{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332cb09",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: OSTEP ì „ì²˜ë¦¬ ë° ì²­í¬ JSON ì €ì¥\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ OSTEP êµì¬ PDFë¥¼ ì „ì²˜ë¦¬í•˜ê³  í† í° ê¸°ë°˜ ì²­í‚¹ í›„ JSONìœ¼ë¡œ ì €ì¥í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "- PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° í’ˆì§ˆ ê´€ë¦¬ í¬ì¸íŠ¸ ì´í•´\n",
    "- ëª©ì°¨(ì±•í„°/íŒŒíŠ¸/ì„œë¸Œì„¹ì…˜) íŒŒì‹±ê³¼ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚° ë°©ë²• ìŠµë“\n",
    "- ë¬¸ì¥ ê²½ê³„ ìœ ì§€í˜• í† í° ê¸°ë°˜ ì²­í‚¹ê³¼ ì˜¤ë²„ë© ì„¤ê³„ ì´í•´\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "- 1ï¸âƒ£ í™˜ê²½ ì„¤ì •: íŒ¨í‚¤ì§€ ì„¤ì¹˜, ê²½ë¡œ/ìƒìˆ˜ ì •ì˜, ë°ì´í„° ìœ„ì¹˜ í™•ì¸\n",
    "- 2ï¸âƒ£ ì„í¬íŠ¸/ìƒìˆ˜: ì •ê·œì‹ íŒ¨í„´, í† í°/ì˜¤ë²„ë©, ë¶„ë¦¬ ê·œì¹™ ë“± ì„¤ì •\n",
    "- 3ï¸âƒ£ PDF ë¡œë“œ/ê¸°ë³¸ í•¨ìˆ˜: í˜ì´ì§€ ì•ˆì „ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹° ì¤€ë¹„\n",
    "- 4ï¸âƒ£ ëª©ì°¨ íŒŒì‹±: ì±•í„°/íŒŒíŠ¸/ì„œë¸Œì„¹ì…˜ êµ¬ì¡°í™”\n",
    "- 5ï¸âƒ£ ë²”ìœ„ ê³„ì‚°/í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì±•í„°ë³„ ë²”ìœ„ ì‚°ì¶œ ë° ì •ì œ\n",
    "- 6ï¸âƒ£ ì²­í¬ ìƒì„±/ì €ì¥: ë¬¸ì¥ ê²½ê³„ ìœ ì§€í˜• í† í° ì²­í‚¹ â†’ JSON ì €ì¥\n",
    "- 7ï¸âƒ£ ê²°ê³¼ ìš”ì•½: ë²”ìœ„/ë¬¸ììˆ˜/ì„œë¸Œì„¹ì…˜ í†µê³„ ì¶œë ¥\n",
    "\n",
    "> âš ï¸ ì‹¤ìŠµ ì…€ ì‹¤í–‰ ì „, í™˜ê²½ ì„¤ì • ì…€(1ï¸âƒ£)ì„ ë¨¼ì € ì‹¤í–‰í•˜ê³  OSTEP PDF ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc101372",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1ï¸âƒ£ Google Colab í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Google Colabì—ì„œ GPUë¥¼ ì‚¬ìš©**í•˜ì—¬ ì‹¤í–‰í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ ì‹¤í–‰ ì „ ì¤€ë¹„ì‚¬í•­\n",
    "1. **ëŸ°íƒ€ì„ ìœ í˜• ì„¤ì •**: ë©”ë‰´ì—ì„œ `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` â†’ `GPU` ì„ íƒ\n",
    "2. **ì²« ë²ˆì§¸ ì½”ë“œ ì…€ ì‹¤í–‰**: Google Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜\n",
    "3. **OSTEP PDF ì—…ë¡œë“œ**: ìµœì´ˆ 1íšŒë§Œ `/content/drive/MyDrive/ostep_rag/data/documents/ostep.pdf` ìœ„ì¹˜ì— ì—…ë¡œë“œ\n",
    "\n",
    "> âš ï¸ **ì¤‘ìš”**: ì•„ë˜ ì½”ë“œ ì…€ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Google Colab í™˜ê²½ ì„¤ì •\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Google Drive ë§ˆìš´íŠ¸\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Google Colab í™˜ê²½ ì„¤ì •\n",
    "# ========================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip -q install PyPDF2 pdfplumber pymupdf tiktoken\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì • (Colab ì „ìš©)\n",
    "BASE_DIR = \"/content/drive/MyDrive/ostep_rag\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "DOC_ID = \"ostep\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"chunk\")\n",
    "PDF_PATH = os.path.join(DATA_DIR, \"documents\", \"ostep.pdf\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(os.path.join(DATA_DIR, \"documents\"), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# OSTEP PDF ì—…ë¡œë“œ í™•ì¸\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"âš ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— OSTEP PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "    print(f\"   {PDF_PATH}\")\n",
    "    print(f\"\\nğŸ“ ì¢Œì¸¡ íŒŒì¼ íƒ­ â†’ drive â†’ MyDrive â†’ ostep_rag â†’ data â†’ documents í´ë”ì— ostep.pdf ì—…ë¡œë“œ\")\n",
    "else:\n",
    "    print(f\"âœ… OSTEP PDF íŒŒì¼ í™•ì¸ë¨: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14830a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2ï¸âƒ£ ì„í¬íŠ¸ ë° ì„¤ì • ìƒìˆ˜\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³  ì „ì—­ ì„¤ì • ìƒìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- PyPDF2ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ì²˜ë¦¬\n",
    "- ì •ê·œì‹ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ëª©ì°¨ì—ì„œ ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ íŒŒì‹±\n",
    "- í† í° ê¸°ë°˜ ì²­í‚¹ ì„¤ì •(ìµœëŒ€ í† í°, ì˜¤ë²„ë© í† í°)\n",
    "- ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° ë¬¸ì„œ ID ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba277ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì„¤ì • ìƒìˆ˜\n",
    "TOC_START_PAGE = 15\n",
    "TOC_END_PAGE = 25\n",
    "PAGE_OFFSET = 36\n",
    "CHUNK_MAX_TOKENS = 400\n",
    "CHUNK_OVERLAP_TOKENS = 80\n",
    "SENTENCE_SPLIT_REGEX = r'(?<=[.!?])\\s+(?=[A-Z0-9])'\n",
    "\n",
    "# ì •ê·œì‹ íŒ¨í„´\n",
    "MAIN_CHAPTER_PATTERNS = [\n",
    "    r'^(\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "PART_PATTERNS = [\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "SUBSECTION_PATTERNS = [\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b647b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ PDF ë¡œë“œ ë° ê¸°ë³¸ í•¨ìˆ˜\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `load_pdf()`: PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ ìˆ˜ë¥¼ í™•ì¸\n",
    "- `extract_pdf_text()`: ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ê³  ì „ì²´ í˜ì´ì§€ ìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053adb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF ë¡œë“œ ì™„ë£Œ: 643 í˜ì´ì§€\n"
     ]
    }
   ],
   "source": [
    "def load_pdf(pdf_path: str):\n",
    "    \"\"\"PDF íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    print(f\"PDF ë¡œë“œ ì™„ë£Œ: {len(reader.pages)} í˜ì´ì§€\")\n",
    "    return reader\n",
    "\n",
    "def extract_pdf_text(reader, start_page: int, end_page: int):\n",
    "    \"\"\"PDF í˜ì´ì§€ ë²”ìœ„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    total_pages = len(reader.pages)\n",
    "    if start_page < 1 or end_page > total_pages:\n",
    "        start_page = max(1, start_page)\n",
    "        end_page = min(total_pages, end_page)\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num in range(start_page - 1, end_page):\n",
    "        page = reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# PDF ë¡œë“œ ì‹¤í–‰\n",
    "pdf_path = PDF_PATH\n",
    "reader = load_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcc01",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ ëª©ì°¨ íŒŒì‹±\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” PDF ëª©ì°¨ë¥¼ íŒŒì‹±í•˜ì—¬ ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `create_toc_entry()`: ëª©ì°¨ í•­ëª©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±\n",
    "- `parse_chapter_line()`, `parse_part_line()`, `parse_subsection_line()`: ê° ìœ í˜•ë³„ ë¼ì¸ íŒŒì‹±\n",
    "- `parse_toc()`: ëª©ì°¨ ì „ì²´ë¥¼ íŒŒì‹±í•˜ì—¬ 3ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ì˜ ê°œìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ëª©ì°¨ì˜ êµ¬ì¡°ë¥¼ ì¸ì‹í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bdba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª©ì°¨ íŒŒì‹± ì¤‘ (í˜ì´ì§€ 15-25)...\n",
      "ëª©ì°¨ íŒŒì‹± ì™„ë£Œ: 50ê°œ ì±•í„°, 3ê°œ íŒŒíŠ¸, 297ê°œ ì„œë¸Œì„¹ì…˜\n"
     ]
    }
   ],
   "source": [
    "def create_toc_entry(entry_type: str, number, title: str, toc_page: int):\n",
    "    \"\"\"TOC í•­ëª© ìƒì„± (ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)\"\"\"\n",
    "    actual_page = toc_page + PAGE_OFFSET\n",
    "    \n",
    "    if entry_type == 'part':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"part_{number}\"\n",
    "    elif entry_type == 'subsection':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"subsec_{number}\"\n",
    "    else:  # chapter\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"ch_{number}\"\n",
    "    \n",
    "    return {\n",
    "        'entry_type': entry_type,\n",
    "        'number': number,\n",
    "        'title': title,\n",
    "        'toc_page': toc_page,\n",
    "        'actual_page': actual_page,\n",
    "        'full_title': full_title,\n",
    "        'id': entry_id\n",
    "    }\n",
    "\n",
    "def clean_toc_title(title: str):\n",
    "    \"\"\"ëª©ì°¨ ì œëª© ì •ì œ\"\"\"\n",
    "    title = re.sub(r'\\s*[\\.\\s]{4,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.{3,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.+\\s*$', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    return title.strip()\n",
    "\n",
    "def is_valid_title(title: str):\n",
    "    \"\"\"ì œëª© ìœ íš¨ì„± ê²€ì‚¬\"\"\"\n",
    "    return (bool(title) and \n",
    "            len(title) > 3 and \n",
    "            not title.strip().isdigit() and \n",
    "            title[0].isupper())\n",
    "\n",
    "def parse_chapter_line(line: str):\n",
    "    \"\"\"ì±•í„° ë¼ì¸ íŒŒì‹±\"\"\"\n",
    "    for pattern in MAIN_CHAPTER_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            chapter_num = int(match.group(1))\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('chapter', chapter_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_part_line(line: str):\n",
    "    \"\"\"íŒŒíŠ¸ ë¼ì¸ íŒŒì‹±\"\"\"\n",
    "    for pattern in PART_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            part_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('part', part_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_subsection_line(line: str):\n",
    "    \"\"\"ì„œë¸Œì„¹ì…˜ ë¼ì¸ íŒŒì‹± (ì˜ˆ: 2.1, 4.2 ë“±)\"\"\"\n",
    "    for pattern in SUBSECTION_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            subsection_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('subsection', subsection_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_toc(reader):\n",
    "    \"\"\"ëª©ì°¨ íŒŒì‹± (ì±•í„° + íŒŒíŠ¸ + ì„œë¸Œì„¹ì…˜)\"\"\"\n",
    "    print(f\"ëª©ì°¨ íŒŒì‹± ì¤‘ (í˜ì´ì§€ {TOC_START_PAGE}-{TOC_END_PAGE})...\")\n",
    "    \n",
    "    toc_text = extract_pdf_text(reader, TOC_START_PAGE, TOC_END_PAGE)\n",
    "    if not toc_text:\n",
    "        print(\"ëª©ì°¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return [], [], []\n",
    "    \n",
    "    chapters = []\n",
    "    parts = []\n",
    "    subsections = []\n",
    "    \n",
    "    for line in toc_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # íŒŒíŠ¸ íŒŒì‹± ìš°ì„  ì‹œë„\n",
    "        part_entry = parse_part_line(line)\n",
    "        if part_entry:\n",
    "            parts.append(part_entry)\n",
    "            continue\n",
    "        \n",
    "        # ì„œë¸Œì„¹ì…˜ íŒŒì‹± ì‹œë„\n",
    "        subsection_entry = parse_subsection_line(line)\n",
    "        if subsection_entry:\n",
    "            subsections.append(subsection_entry)\n",
    "            continue\n",
    "        \n",
    "        # ì±•í„° íŒŒì‹±\n",
    "        chapter_entry = parse_chapter_line(line)\n",
    "        if chapter_entry:\n",
    "            chapters.append(chapter_entry)\n",
    "    \n",
    "    # í˜ì´ì§€ ìˆœì„œë¡œ ì •ë ¬\n",
    "    chapters.sort(key=lambda x: x['toc_page'])\n",
    "    parts.sort(key=lambda x: x['toc_page'])\n",
    "    subsections.sort(key=lambda x: x['toc_page'])\n",
    "    \n",
    "    print(f\"ëª©ì°¨ íŒŒì‹± ì™„ë£Œ: {len(chapters)}ê°œ ì±•í„°, {len(parts)}ê°œ íŒŒíŠ¸, {len(subsections)}ê°œ ì„œë¸Œì„¹ì…˜\")\n",
    "    return chapters, parts, subsections\n",
    "\n",
    "# ëª©ì°¨ íŒŒì‹± ì‹¤í–‰\n",
    "chapters, parts, subsections = parse_toc(reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee5e56",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ ì±•í„° ë²”ìœ„ ê³„ì‚°\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ê° ì±•í„°ê°€ ì°¨ì§€í•˜ëŠ” í˜ì´ì§€ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `find_part_for_chapter()`: ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ë¥¼ ì°¾ê¸°\n",
    "- `calculate_chapter_ranges()`: ì±•í„°ë³„ ì‹œì‘/ì¢…ë£Œ í˜ì´ì§€ë¥¼ ê³„ì‚°\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì˜ í˜ì´ì§€ ë²”ìœ„ê°€ ê³„ì‚°ë©ë‹ˆë‹¤.\n",
    "- ë‹¤ìŒ ì±•í„° ì‹œì‘ ì „ê¹Œì§€ê°€ í˜„ì¬ ì±•í„°ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚° ì¤‘...\n",
      "ì±•í„° ë²”ìœ„ ê³„ì‚° ì™„ë£Œ: 50ê°œ ì±•í„°\n"
     ]
    }
   ],
   "source": [
    "def find_part_for_chapter(chapter, parts):\n",
    "    \"\"\"ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ ì°¾ê¸°\"\"\"\n",
    "    current_part = None\n",
    "    for part in parts:\n",
    "        if part['actual_page'] <= chapter['actual_page']:\n",
    "            current_part = part\n",
    "        else:\n",
    "            break\n",
    "    return current_part\n",
    "\n",
    "def calculate_chapter_ranges(reader, chapters, parts):\n",
    "    \"\"\"ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°\"\"\"\n",
    "    print(\"ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚° ì¤‘...\")\n",
    "    \n",
    "    if not chapters:\n",
    "        print(\"ë©”ì¸ ì±•í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    chapter_ranges = []\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    for i, chapter in enumerate(chapters):\n",
    "        start_page = chapter['actual_page']\n",
    "        \n",
    "        # ë‹¤ìŒ ì±•í„°ê°€ ìˆìœ¼ë©´ ê·¸ ì „ í˜ì´ì§€ê¹Œì§€, ì—†ìœ¼ë©´ PDF ëê¹Œì§€\n",
    "        if i + 1 < len(chapters):\n",
    "            end_page = chapters[i + 1]['actual_page'] - 1\n",
    "        else:\n",
    "            end_page = total_pages\n",
    "        \n",
    "        # í•´ë‹¹ ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ ì°¾ê¸°\n",
    "        part_info = find_part_for_chapter(chapter, parts)\n",
    "        \n",
    "        # í˜ì´ì§€ ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬\n",
    "        if start_page <= end_page:\n",
    "            chapter_range = {\n",
    "                'toc_entry': chapter,\n",
    "                'part_info': part_info,\n",
    "                'start_page': start_page,\n",
    "                'end_page': end_page,\n",
    "                'page_count': end_page - start_page + 1\n",
    "            }\n",
    "            chapter_ranges.append(chapter_range)\n",
    "    \n",
    "    print(f\"ì±•í„° ë²”ìœ„ ê³„ì‚° ì™„ë£Œ: {len(chapter_ranges)}ê°œ ì±•í„°\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# ì±•í„° ë²”ìœ„ ê³„ì‚° ì‹¤í–‰\n",
    "chapter_ranges = calculate_chapter_ranges(reader, chapters, parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f526e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ê° ì±•í„°ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì„œë¸Œì„¹ì…˜ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `clean_text()`: PDF í—¤ë”/í‘¸í„°, í˜ì´ì§€ ë²ˆí˜¸ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "- `find_chapter_subsections()`: ì±•í„°ì— ì†í•œ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\n",
    "- `split_text_by_subsections()`: í…ìŠ¤íŠ¸ë¥¼ ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• \n",
    "- `extract_texts()`: ëª¨ë“  ì±•í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì˜ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ê³  ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\n",
    "- í—¤ë”/í‘¸í„° ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œê°€ ì œê±°ëœ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì¤‘...\n",
      "ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì™„ë£Œ: 50ê°œ ì±•í„°\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ì œ ë° í—¤ë”/í‘¸í„° ì œê±°\"\"\"\n",
    "    # PDF í—¤ë”/í‘¸í„° ì œê±°\n",
    "    header_pattern = r'c/circle\\s*copyrt\\s*\\d+,?\\s*A\\s+RPACI\\s*-?\\s*D\\s*USSEAU\\s*THREE\\s+EASY\\s+PIECES'\n",
    "    text = re.sub(header_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    footer_pattern = r'OPERATING\\s+SYSTEMS\\s+\\[VERSION\\s+[\\d.]+\\]\\s+WWW\\s*\\.\\s*OSTEP\\s*\\.\\s*ORG.*$'\n",
    "    text = re.sub(footer_pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±°\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_chapter_subsections(chapter, subsections):\n",
    "    \"\"\"ì±•í„°ì— ì†í•œ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\"\"\"\n",
    "    chapter_num = str(chapter['number'])\n",
    "    chapter_subsections = []\n",
    "    \n",
    "    for subsection in subsections:\n",
    "        # ì„œë¸Œì„¹ì…˜ ë²ˆí˜¸ê°€ í•´ë‹¹ ì±•í„°ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: 2.1, 2.2ëŠ” ì±•í„° 2ì— ì†í•¨)\n",
    "        subsection_num = str(subsection['number'])\n",
    "        if subsection_num.startswith(f\"{chapter_num}.\"):\n",
    "            chapter_subsections.append(subsection)\n",
    "    \n",
    "    # ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    chapter_subsections.sort(key=lambda x: float(x['number']))\n",
    "    return chapter_subsections\n",
    "\n",
    "def build_flexible_title_pattern(title: str):\n",
    "    \"\"\"ì œëª©ì„ ì•ˆì „í•œ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    parts = []\n",
    "    for ch in title:\n",
    "        if ch.isspace():\n",
    "            parts.append(r\"\\s+\")\n",
    "        elif ch == '(':\n",
    "            parts.append(r\"\\s*\\(\\s*\")\n",
    "        elif ch == ')':\n",
    "            parts.append(r\"\\s*\\)\\s*\")\n",
    "        else:\n",
    "            parts.append(re.escape(ch))\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_text_by_subsections(chapter_text: str, chapter, chapter_subsections):\n",
    "    \"\"\"ì±•í„° í…ìŠ¤íŠ¸ë¥¼ ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• \"\"\"\n",
    "    if not chapter_subsections:\n",
    "        return []\n",
    "    \n",
    "    subsection_data_list = []\n",
    "    \n",
    "    split_points = []\n",
    "    \n",
    "    for subsection in chapter_subsections:\n",
    "        section_title = f\"{subsection['number']} {subsection['title']}\"\n",
    "        \n",
    "        patterns_to_try = [\n",
    "            re.escape(section_title),\n",
    "            f\"{re.escape(str(subsection['number']))}\\\\s+{build_flexible_title_pattern(subsection['title'])}\"\n",
    "        ]\n",
    "\n",
    "        # \"The\" ì‹œì‘í•˜ëŠ” ì œëª©ì˜ íŠ¹ìˆ˜ ì²˜ë¦¬\n",
    "        lower_title = subsection['title'].lower()\n",
    "        if lower_title.startswith(\"the \"):\n",
    "            after_the = subsection['title'][4:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        elif lower_title.startswith(\"the\"):\n",
    "            after_the = subsection['title'][3:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        \n",
    "        found = False\n",
    "        for pattern in patterns_to_try:\n",
    "            matches = list(re.finditer(pattern, chapter_text, re.IGNORECASE | re.MULTILINE))\n",
    "            if matches:\n",
    "                # ì²« ë²ˆì§¸ ë§¤ì¹˜ ì‚¬ìš©\n",
    "                match = matches[0]\n",
    "                split_points.append({\n",
    "                    'subsection': subsection,\n",
    "                    'start_pos': match.start(),\n",
    "                    'end_pos': match.end(),\n",
    "                    'title_match': match.group().strip()\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"  ì„œë¸Œì„¹ì…˜ '{subsection['number']} {subsection['title']}' ì œëª©ì„ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ìœ„ì¹˜ë³„ë¡œ ì •ë ¬\n",
    "    split_points.sort(key=lambda x: x['start_pos'])\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    for i, split_point in enumerate(split_points):\n",
    "        start_pos = split_point['start_pos']\n",
    "        \n",
    "        # ë‹¤ìŒ ì„œë¸Œì„¹ì…˜ì˜ ì‹œì‘ ìœ„ì¹˜ ë˜ëŠ” í…ìŠ¤íŠ¸ ë\n",
    "        if i + 1 < len(split_points):\n",
    "            end_pos = split_points[i + 1]['start_pos']\n",
    "        else:\n",
    "            end_pos = len(chapter_text)\n",
    "        \n",
    "        # ì„œë¸Œì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        subsection_text = chapter_text[start_pos:end_pos].strip()\n",
    "        \n",
    "        if subsection_text:\n",
    "            subsection_data = {\n",
    "                'toc_entry': split_point['subsection'],\n",
    "                'parent_chapter': chapter,\n",
    "                'text': clean_text(subsection_text)\n",
    "            }\n",
    "            subsection_data_list.append(subsection_data)\n",
    "    \n",
    "    return subsection_data_list\n",
    "\n",
    "def extract_texts(reader, chapter_ranges, subsections):\n",
    "    \"\"\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \"\"\"\n",
    "    print(f\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì¤‘...\")\n",
    "    for chapter_range in chapter_ranges:\n",
    "        chapter = chapter_range['toc_entry']\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        text = extract_pdf_text(reader, chapter_range['start_page'], chapter_range['end_page'])\n",
    "        chapter_range['text'] = clean_text(text)\n",
    "        \n",
    "        # í•´ë‹¹ ì±•í„°ì˜ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\n",
    "        chapter_subsections = find_chapter_subsections(chapter, subsections)\n",
    "        \n",
    "        if chapter_subsections:\n",
    "            # ì„œë¸Œì„¹ì…˜ë³„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "            subsection_data_list = split_text_by_subsections(chapter_range['text'], chapter, chapter_subsections)\n",
    "            chapter_range['subsections'] = subsection_data_list\n",
    "        else:\n",
    "            chapter_range['subsections'] = []\n",
    "    \n",
    "    print(f\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì™„ë£Œ: {len(chapter_ranges)}ê°œ ì±•í„°\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì‹¤í–‰\n",
    "chapter_ranges = extract_texts(reader, chapter_ranges, subsections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b194f",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ ì²­í¬ ìƒì„± ë° JSON ì €ì¥\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `estimate_tokens_length()`: tiktoken ë˜ëŠ” ê·¼ì‚¬ì¹˜ë¡œ í† í° ìˆ˜ ì¶”ì •\n",
    "- `split_text_into_sentences()`: ì •ê·œì‹ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬\n",
    "- `chunk_sentences_hybrid()`: ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° í† í° ì œí•œìœ¼ë¡œ ì²­í‚¹\n",
    "- `build_all_chunk_records()`: ëª¨ë“  ì²­í¬ë¥¼ ë ˆì½”ë“œë¡œ ìƒì„±\n",
    "- `write_json()`: JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° ì œí•œ(400í† í°, 20% ì˜¤ë²„ë©)ì— ë§ì¶° ì²­í‚¹ë©ë‹ˆë‹¤.\n",
    "- JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b6d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5ë‹¨ê³„: ì²­í¬ ìƒì„± ë° ì €ì¥(JSON)\n",
      "----------------------------------------\n",
      "ì²­í¬ ì €ì¥ ì™„ë£Œ: ../data/chunk/ostep_tok400_ov20.json (991 chunks)\n"
     ]
    }
   ],
   "source": [
    "def ensure_output_dir(output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def build_output_path(output_dir: str, max_tokens: int, overlap_tokens: int):\n",
    "    overlap_pct = int(round((overlap_tokens / max_tokens) * 100)) if max_tokens > 0 else 0\n",
    "    filename = f\"{DOC_ID}_tok{max_tokens}_ov{overlap_pct}.json\"\n",
    "    return str(Path(output_dir) / filename)\n",
    "\n",
    "def try_import_tiktoken():\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def estimate_tokens_length(text: str):\n",
    "    \"\"\"í† í° ê¸¸ì´ ì¶”ì • (tiktoken ìš°ì„ , ì—†ìœ¼ë©´ ê·¼ì‚¬ì¹˜)\"\"\"\n",
    "    tiktoken = try_import_tiktoken()\n",
    "    if tiktoken is not None:\n",
    "        try:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except Exception:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "    return int(len(text.split()) * 1.3)\n",
    "\n",
    "def split_text_into_sentences(text: str):\n",
    "    \"\"\"ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    sentences = re.split(SENTENCE_SPLIT_REGEX, text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def chunk_sentences_hybrid(sentences, max_tokens: int, overlap_tokens: int):\n",
    "    \"\"\"ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° í† í° ìƒí•œìœ¼ë¡œ ì²­í‚¹\"\"\"\n",
    "    chunks = []\n",
    "    if not sentences:\n",
    "        return chunks\n",
    "\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    for sent in sentences:\n",
    "        sent_tokens = estimate_tokens_length(sent)\n",
    "        \n",
    "        if sent_tokens > max_tokens:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "                current = []\n",
    "                current_tokens = 0\n",
    "            chunks.append(sent.strip())\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sent_tokens <= max_tokens:\n",
    "            current.append(sent)\n",
    "            current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "            \n",
    "            if overlap_tokens > 0:\n",
    "                overlap_bucket = []\n",
    "                overlap_count = 0\n",
    "                for prev_sent in reversed(current):\n",
    "                    t = estimate_tokens_length(prev_sent)\n",
    "                    if overlap_count + t > overlap_tokens:\n",
    "                        break\n",
    "                    overlap_bucket.append(prev_sent)\n",
    "                    overlap_count += t\n",
    "                overlap_bucket.reverse()\n",
    "                current = overlap_bucket + [sent]\n",
    "                current_tokens = sum(estimate_tokens_length(s) for s in current)\n",
    "            else:\n",
    "                current = [sent]\n",
    "                current_tokens = sent_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current).strip())\n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(chapter_id: str, subsec_id, chunk_index: int):\n",
    "    base = f\"{chapter_id}\"\n",
    "    if subsec_id:\n",
    "        base += f\"__{subsec_id}\"\n",
    "    return f\"{base}__{chunk_index:04d}\"\n",
    "\n",
    "def find_part_for_page(page: int, part_lookup_by_page):\n",
    "\tcurrent = None\n",
    "\tfor start_page, part in part_lookup_by_page:\n",
    "\t\tif start_page <= page:\n",
    "\t\t\tcurrent = part\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\treturn current\n",
    "\n",
    "def build_all_chunk_records(pdf_path: str, parts, chapter_ranges, max_tokens: int, overlap_tokens: int):\n",
    "    part_lookup_by_page = []\n",
    "    for p in parts:\n",
    "        part_lookup_by_page.append((p['actual_page'], p))\n",
    "    part_lookup_by_page.sort(key=lambda x: x[0])\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for cr in chapter_ranges:\n",
    "        chapter = cr['toc_entry']\n",
    "        chapter_id = chapter['id']\n",
    "        chapter_title = chapter['title']\n",
    "        part_info = cr.get('part_info')\n",
    "        if part_info is None:\n",
    "            part_info = find_part_for_page(cr['start_page'], part_lookup_by_page)\n",
    "\n",
    "        # ì„œë¸Œì„¹ì…˜ì´ ìˆìœ¼ë©´ ì„œë¸Œì„¹ì…˜ ê¸°ì¤€, ì—†ìœ¼ë©´ ì±•í„° ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹\n",
    "        if cr.get('subsections'):\n",
    "            for sub in cr['subsections']:\n",
    "                subsec = sub\n",
    "                sentences = split_text_into_sentences(subsec['text'])\n",
    "                chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "                for idx, chunk_text in enumerate(chunks):\n",
    "                    records.append({\n",
    "                        'chunk_id': make_chunk_id(chapter_id, subsec['toc_entry']['id'], idx),\n",
    "                        'chapter_id': chapter_id,\n",
    "                        'chapter_title': chapter_title,\n",
    "                        'subsection_id': subsec['toc_entry']['id'],\n",
    "                        'subsection_title': f\"{subsec['toc_entry']['number']} {subsec['toc_entry']['title']}\",\n",
    "                        'text': chunk_text,\n",
    "                    })\n",
    "        else:\n",
    "            sentences = split_text_into_sentences(cr['text'])\n",
    "            chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                records.append({\n",
    "                    'chunk_id': make_chunk_id(chapter_id, None, idx),\n",
    "                    'chapter_id': chapter_id,\n",
    "                    'chapter_title': chapter_title,\n",
    "                    'subsection_id': None,\n",
    "                    'subsection_title': None,\n",
    "                    'text': chunk_text,\n",
    "                })\n",
    "\n",
    "    return records\n",
    "\n",
    "def write_json(path: str, records):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ì²­í¬ ìƒì„± ë° ì €ì¥\n",
    "print(\"\\n5ë‹¨ê³„: ì²­í¬ ìƒì„± ë° ì €ì¥(JSON)\")\n",
    "print(\"-\" * 40)\n",
    "ensure_output_dir(OUTPUT_DIR)\n",
    "output_path = build_output_path(OUTPUT_DIR, CHUNK_MAX_TOKENS, CHUNK_OVERLAP_TOKENS)\n",
    "all_chunk_records = build_all_chunk_records(\n",
    "\tpdf_path=pdf_path,\n",
    "\tparts=parts,\n",
    "\tchapter_ranges=chapter_ranges,\n",
    "\tmax_tokens=CHUNK_MAX_TOKENS,\n",
    "\toverlap_tokens=CHUNK_OVERLAP_TOKENS,\n",
    ")\n",
    "write_json(output_path, all_chunk_records)\n",
    "print(f\"ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_path} ({len(all_chunk_records)} chunks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9d4bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 8ï¸âƒ£ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„, ë¬¸ì ìˆ˜, ì„œë¸Œì„¹ì…˜ ê°œìˆ˜\n",
    "- ì„œë¸Œì„¹ì…˜ë³„ ë¬¸ì ìˆ˜\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì™€ ì„œë¸Œì„¹ì…˜ì˜ ìƒì„¸ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ì „ì²˜ë¦¬ í’ˆì§ˆì„ í™•ì¸í•˜ê³  ê°œì„ ì ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter information:\n",
      "  â€¢ 1 A Dialogue on the Book (Unclassified): 37-38 (2p, 3,519 chars, 0 subsections)\n",
      "  â€¢ 2 Introduction to Operating Systems (Unclassified): 39-58 (20p, 44,104 chars, 7 subsections)\n",
      "    â”œâ”€ 2.1 Virtualizing the CPU (3,750 chars)\n",
      "    â”œâ”€ 2.2 Virtualizing Memory (3,052 chars)\n",
      "    â”œâ”€ 2.3 Concurrency (5,386 chars)\n",
      "    â”œâ”€ 2.4 Persistence (4,965 chars)\n",
      "    â”œâ”€ 2.5 Design Goals (3,919 chars)\n",
      "    â”œâ”€ 2.6 Some History (11,916 chars)\n",
      "    â”œâ”€ 2.7 Summary (5,596 chars)\n",
      "  â€¢ 3 A Dialogue on Virtualization (Part I): 59-60 (2p, 2,285 chars, 0 subsections)\n",
      "  â€¢ 4 The Abstraction: The Process (Part I): 61-70 (10p, 18,608 chars, 6 subsections)\n",
      "    â”œâ”€ 4.1 The Abstraction: A Process (2,252 chars)\n",
      "    â”œâ”€ 4.2 Process API (1,628 chars)\n",
      "    â”œâ”€ 4.3 Process Creation: A Little More Detail (3,449 chars)\n",
      "    â”œâ”€ 4.4 Process States (1,697 chars)\n",
      "    â”œâ”€ 4.5 Data Structures (4,060 chars)\n",
      "    â”œâ”€ 4.6 Summary (2,061 chars)\n",
      "  â€¢ 5 Interlude: Process API (Part I): 71-80 (10p, 19,141 chars, 6 subsections)\n",
      "    â”œâ”€ 5.1 Thefork() System Call (4,566 chars)\n",
      "    â”œâ”€ 5.2 Adding wait() System Call (1,360 chars)\n",
      "    â”œâ”€ 5.3 Finally, the exec() System Call (2,877 chars)\n",
      "    â”œâ”€ 5.4 Why? Motivating the API (6,564 chars)\n",
      "    â”œâ”€ 5.5 Other Parts of the API (1,292 chars)\n",
      "    â”œâ”€ 5.6 Summary (1,453 chars)\n",
      "  â€¢ 6 Mechanism: Limited Direct Execution (Part I): 81-94 (14p, 34,593 chars, 5 subsections)\n",
      "    â”œâ”€ 6.1 Basic Technique: Limited Direct Execution (1,980 chars)\n",
      "    â”œâ”€ 6.2 Problem #1: Restricted Operations (9,648 chars)\n",
      "    â”œâ”€ 6.3 Problem #2: Switching Between Processes (10,566 chars)\n",
      "    â”œâ”€ 6.4 Worried About Concurrency? (2,635 chars)\n",
      "    â”œâ”€ 6.5 Summary (8,439 chars)\n",
      "  â€¢ 7 Scheduling: Introduction (Part I): 95-106 (12p, 25,830 chars, 9 subsections)\n",
      "    â”œâ”€ 7.1 Workload Assumptions (1,350 chars)\n",
      "    â”œâ”€ 7.2 Scheduling Metrics (1,281 chars)\n",
      "    â”œâ”€ 7.3 First In, First Out (FIFO) (3,167 chars)\n",
      "    â”œâ”€ 7.4 Shortest Job First (SJF) (2,888 chars)\n",
      "    â”œâ”€ 7.5 Shortest Time-to-Completion First (STCF) (3,034 chars)\n",
      "    â”œâ”€ 7.6 Round Robin (4,457 chars)\n",
      "    â”œâ”€ 7.7 Incorporating I/O (3,253 chars)\n",
      "    â”œâ”€ 7.8 No More Oracle (566 chars)\n",
      "    â”œâ”€ 7.9 Summary (4,702 chars)\n",
      "  â€¢ 8 Scheduling:The Multi-Level Feedback Queue (Part I): 107-118 (12p, 23,348 chars, 6 subsections)\n",
      "    â”œâ”€ 8.1 MLFQ: Basic Rules (2,444 chars)\n",
      "    â”œâ”€ 8.2 Attempt #1: How to Change Priority (5,593 chars)\n",
      "    â”œâ”€ 8.3 Attempt #2: The Priority Boost (2,124 chars)\n",
      "    â”œâ”€ 8.4 Attempt #3: Better Accounting (1,491 chars)\n",
      "    â”œâ”€ 8.5 Tuning MLFQ And Other Issues (3,499 chars)\n",
      "    â”œâ”€ 8.6 MLFQ: Summary (5,847 chars)\n",
      "  â€¢ 9 Scheduling: Proportional Share (Part I): 119-128 (10p, 20,180 chars, 7 subsections)\n",
      "    â”œâ”€ 9.1 Basic Concept: Tickets Represent Your Share (3,834 chars)\n",
      "    â”œâ”€ 9.2 Ticket Mechanisms (2,955 chars)\n",
      "    â”œâ”€ 9.3 Implementation (2,088 chars)\n",
      "    â”œâ”€ 9.4 An Example (1,321 chars)\n",
      "    â”œâ”€ 9.5 How To Assign Tickets? (581 chars)\n",
      "    â”œâ”€ 9.6 Why Not Deterministic? (3,862 chars)\n",
      "    â”œâ”€ 9.7 Summary (4,491 chars)\n",
      "  â€¢ 10 Multiprocessor Scheduling (Advanced) (Part I): 129-140 (12p, 24,645 chars, 7 subsections)\n",
      "    â”œâ”€ 10.1 Background: Multiprocessor Architecture (4,725 chars)\n",
      "    â”œâ”€ 10.2 Donâ€™t Forget Synchronization (2,882 chars)\n",
      "    â”œâ”€ 10.3 One Final Issue: Cache Afï¬nity (879 chars)\n",
      "    â”œâ”€ 10.4 Single-Queue Scheduling (3,189 chars)\n",
      "    â”œâ”€ 10.5 Multi-Queue Scheduling (5,487 chars)\n",
      "    â”œâ”€ 10.6 Linux Multiprocessor Schedulers (1,338 chars)\n",
      "    â”œâ”€ 10.7 Summary (3,533 chars)\n",
      "  â€¢ 11 Summary Dialogue on CPU Virtualization (Part I): 141-142 (2p, 4,045 chars, 0 subsections)\n",
      "  â€¢ 12 A Dialogue on Memory Virtualization (Part I): 143-144 (2p, 3,288 chars, 0 subsections)\n",
      "  â€¢ 13 The Abstraction: Address Spaces (Part I): 145-154 (10p, 17,805 chars, 5 subsections)\n",
      "    â”œâ”€ 13.1 Early Systems (1,043 chars)\n",
      "    â”œâ”€ 13.2 Multiprogramming and Time Sharing (2,842 chars)\n",
      "    â”œâ”€ 13.3 The Address Space (4,628 chars)\n",
      "    â”œâ”€ 13.4 Goals (4,650 chars)\n",
      "    â”œâ”€ 13.5 Summary (4,248 chars)\n",
      "  â€¢ 14 Interlude: Memory API (Part I): 155-164 (10p, 18,359 chars, 7 subsections)\n",
      "    â”œâ”€ 14.1 Types of Memory (2,177 chars)\n",
      "    â”œâ”€ 14.2 Themalloc() Call (4,160 chars)\n",
      "    â”œâ”€ 14.3 Thefree() Call (503 chars)\n",
      "    â”œâ”€ 14.4 Common Errors (6,884 chars)\n",
      "    â”œâ”€ 14.5 Underlying OS Support (1,527 chars)\n",
      "    â”œâ”€ 14.6 Other Calls (618 chars)\n",
      "    â”œâ”€ 14.7 Summary (2,019 chars)\n",
      "  â€¢ 15 Mechanism: Address Translation (Part I): 165-176 (12p, 25,520 chars, 5 subsections)\n",
      "    â”œâ”€ 15.1 Assumptions (886 chars)\n",
      "    â”œâ”€ 15.2 An Example (4,155 chars)\n",
      "    â”œâ”€ 15.3 Dynamic (Hardware-based) Relocation (7,338 chars)\n",
      "    â”œâ”€ 15.4 OS Issues (3,512 chars)\n",
      "    â”œâ”€ 15.5 Summary (5,958 chars)\n",
      "  â€¢ 16 Segmentation (Part I): 177-188 (12p, 23,918 chars, 7 subsections)\n",
      "    â”œâ”€ 16.1 Segmentation: Generalized Base/Bounds (3,999 chars)\n",
      "    â”œâ”€ 16.2 Which Segment Are We Referring To? (3,169 chars)\n",
      "    â”œâ”€ 16.3 What About The Stack? (1,894 chars)\n",
      "    â”œâ”€ 16.4 Support for Sharing (1,838 chars)\n",
      "    â”œâ”€ 16.5 Fine-grained vs. Coarse-grained Segmentation (1,187 chars)\n",
      "    â”œâ”€ 16.6 OS Support (4,052 chars)\n",
      "    â”œâ”€ 16.7 Summary (6,440 chars)\n",
      "  â€¢ 17 Free-Space Management (Part I): 189-204 (16p, 31,637 chars, 5 subsections)\n",
      "    â”œâ”€ 17.1 Assumptions (3,087 chars)\n",
      "    â”œâ”€ 17.2 Low-level Mechanisms (13,270 chars)\n",
      "    â”œâ”€ 17.3 Basic Strategies (4,325 chars)\n",
      "    â”œâ”€ 17.4 Other Approaches (6,329 chars)\n",
      "    â”œâ”€ 17.5 Summary (2,619 chars)\n",
      "  â€¢ 18 Paging: Introduction (Part I): 205-218 (14p, 25,674 chars, 5 subsections)\n",
      "    â”œâ”€ 18.1 Where Are Page Tables Stored? (1,665 chars)\n",
      "    â”œâ”€ 18.2 Whatâ€™s Actually In The Page Table? (3,461 chars)\n",
      "    â”œâ”€ 18.3 Paging: Also Too Slow (4,367 chars)\n",
      "    â”œâ”€ 18.4 A Memory Trace (5,549 chars)\n",
      "    â”œâ”€ 18.5 Summary (4,348 chars)\n",
      "  â€¢ 19 Paging: Faster Translations (TLBs) (Part I): 219-236 (18p, 39,859 chars, 8 subsections)\n",
      "    â”œâ”€ 19.1 TLB Basic Algorithm (3,253 chars)\n",
      "    â”œâ”€ 19.2 Example: Accessing An Array (6,331 chars)\n",
      "    â”œâ”€ 19.3 Who Handles The TLB Miss? (6,051 chars)\n",
      "    â”œâ”€ 19.4 TLB Contents: Whatâ€™s In There? (2,408 chars)\n",
      "    â”œâ”€ 19.5 TLB Issue: Context Switches (4,565 chars)\n",
      "    â”œâ”€ 19.6 Issue: Replacement Policy (1,439 chars)\n",
      "    â”œâ”€ 19.7 A Real TLB Entry (3,659 chars)\n",
      "    â”œâ”€ 19.8 Summary (10,500 chars)\n",
      "  â€¢ 20 Paging: Smaller Tables (Part I): 237-252 (16p, 33,311 chars, 6 subsections)\n",
      "    â”œâ”€ 20.1 Simple Solution: Bigger Pages (2,493 chars)\n",
      "    â”œâ”€ 20.2 Hybrid Approach: Paging and Segments (6,492 chars)\n",
      "    â”œâ”€ 20.3 Multi-level Page Tables (17,351 chars)\n",
      "    â”œâ”€ 20.4 Inverted Page Tables (1,022 chars)\n",
      "    â”œâ”€ 20.5 Swapping the Page Tables to Disk (675 chars)\n",
      "    â”œâ”€ 20.6 Summary (4,098 chars)\n",
      "  â€¢ 21 Beyond Physical Memory: Mechanisms (Part I): 253-262 (10p, 22,168 chars, 7 subsections)\n",
      "    â”œâ”€ 21.1 Swap Space (2,297 chars)\n",
      "    â”œâ”€ 21.2 The Present Bit (3,628 chars)\n",
      "    â”œâ”€ 21.3 The Page Fault (2,943 chars)\n",
      "    â”œâ”€ 21.4 What If Memory Is Full? (1,935 chars)\n",
      "    â”œâ”€ 21.5 Page Fault Control Flow (2,503 chars)\n",
      "    â”œâ”€ 21.6 When Replacements Really Occur (2,928 chars)\n",
      "    â”œâ”€ 21.7 Summary (2,723 chars)\n",
      "  â€¢ 22 Beyond Physical Memory: Policies (Part I): 263-280 (18p, 37,801 chars, 12 subsections)\n",
      "    â”œâ”€ 22.1 Cache Management (2,979 chars)\n",
      "    â”œâ”€ 22.2 The Optimal Replacement Policy (5,077 chars)\n",
      "    â”œâ”€ 22.3 A Simple Policy: FIFO (3,293 chars)\n",
      "    â”œâ”€ 22.4 Another Simple Policy: Random (1,458 chars)\n",
      "    â”œâ”€ 22.5 Using History: LRU (4,103 chars)\n",
      "    â”œâ”€ 22.6 Workload Examples (4,514 chars)\n",
      "    â”œâ”€ 22.7 Implementing Historical Algorithms (2,245 chars)\n",
      "    â”œâ”€ 22.8 Approximating LRU (2,737 chars)\n",
      "    â”œâ”€ 22.9 Considering Dirty Pages (1,013 chars)\n",
      "    â”œâ”€ 22.10 Other VM Policies (1,327 chars)\n",
      "    â”œâ”€ 22.11 Thrashing (1,463 chars)\n",
      "    â”œâ”€ 22.12 Summary (6,463 chars)\n",
      "  â€¢ 23 The V AX/VMS Virtual Memory System (Part I): 281-290 (10p, 20,348 chars, 6 subsections)\n",
      "    â”œâ”€ 23.1 Background (2,013 chars)\n",
      "    â”œâ”€ 23.2 Memory Management Hardware (3,112 chars)\n",
      "    â”œâ”€ 23.3 A Real Address Space (3,263 chars)\n",
      "    â”œâ”€ 23.4 Page Replacement (4,141 chars)\n",
      "    â”œâ”€ 23.5 Other Neat VM Tricks (4,217 chars)\n",
      "    â”œâ”€ 23.6 Summary (3,223 chars)\n",
      "  â€¢ 24 Summary Dialogue on Memory Virtualization (Part I): 291-296 (6p, 5,378 chars, 0 subsections)\n",
      "  â€¢ 25 A Dialogue on Concurrency (Part II): 297-298 (2p, 3,069 chars, 0 subsections)\n",
      "  â€¢ 26 Concurrency: An Introduction (Part II): 299-314 (16p, 30,634 chars, 6 subsections)\n",
      "    â”œâ”€ 26.1 An Example: Thread Creation (4,711 chars)\n",
      "    â”œâ”€ 26.2 Why It Gets Worse: Shared Data (3,351 chars)\n",
      "    â”œâ”€ 26.3 The Heart of the Problem: Uncontrolled Scheduling (4,903 chars)\n",
      "    â”œâ”€ 26.4 The Wish For Atomicity (3,711 chars)\n",
      "    â”œâ”€ 26.5 One More Problem: Waiting For Another (1,091 chars)\n",
      "    â”œâ”€ 26.6 Summary: Why in OS Class? (8,921 chars)\n",
      "  â€¢ 27 Interlude: Thread API (Part II): 315-326 (12p, 22,504 chars, 6 subsections)\n",
      "    â”œâ”€ 27.1 Thread Creation (2,916 chars)\n",
      "    â”œâ”€ 27.2 Thread Completion (5,594 chars)\n",
      "    â”œâ”€ 27.3 Locks (4,440 chars)\n",
      "    â”œâ”€ 27.4 Condition Variables (4,280 chars)\n",
      "    â”œâ”€ 27.5 Compiling and Running (596 chars)\n",
      "    â”œâ”€ 27.6 Summary (4,032 chars)\n",
      "  â€¢ 28 Locks (Part II): 327-346 (20p, 46,153 chars, 17 subsections)\n",
      "    â”œâ”€ 28.1 Locks: The Basic Idea (2,692 chars)\n",
      "    â”œâ”€ 28.2 Pthread Locks (1,106 chars)\n",
      "    â”œâ”€ 28.3 Building A Lock (1,107 chars)\n",
      "    â”œâ”€ 28.4 Evaluating Locks (1,441 chars)\n",
      "    â”œâ”€ 28.5 Controlling Interrupts (4,638 chars)\n",
      "    â”œâ”€ 28.6 Test And Set (Atomic Exchange) (3,838 chars)\n",
      "    â”œâ”€ 28.7 Building A Working Spin Lock (3,798 chars)\n",
      "    â”œâ”€ 28.8 Evaluating Spin Locks (2,041 chars)\n",
      "    â”œâ”€ 28.9 Compare-And-Swap (2,170 chars)\n",
      "    â”œâ”€ 28.10 Load-Linked and Store-Conditional (4,106 chars)\n",
      "    â”œâ”€ 28.11 Fetch-And-Add (1,983 chars)\n",
      "    â”œâ”€ 28.12 Summary: So Much Spinning (1,470 chars)\n",
      "    â”œâ”€ 28.13 A Simple Approach: Just Yield, Baby (2,359 chars)\n",
      "    â”œâ”€ 28.14 Using Queues: Sleeping Instead Of Spinning (4,752 chars)\n",
      "    â”œâ”€ 28.15 Different OS, Different Support (1,718 chars)\n",
      "    â”œâ”€ 28.16 Two-Phase Locks (2,003 chars)\n",
      "    â”œâ”€ 28.17 Summary (4,294 chars)\n",
      "  â€¢ 29 Lock-based Concurrent Data Structures (Part II): 347-360 (14p, 25,894 chars, 5 subsections)\n",
      "    â”œâ”€ 29.1 Concurrent Counters (8,804 chars)\n",
      "    â”œâ”€ 29.2 Concurrent Linked Lists (6,469 chars)\n",
      "    â”œâ”€ 29.3 Concurrent Queues (2,524 chars)\n",
      "    â”œâ”€ 29.4 Concurrent Hash Table (2,816 chars)\n",
      "    â”œâ”€ 29.5 Summary (4,123 chars)\n",
      "  â€¢ 30 Condition Variables (Part II): 361-376 (16p, 31,751 chars, 4 subsections)\n",
      "    â”œâ”€ 30.1 Deï¬nition and Routines (6,850 chars)\n",
      "    â”œâ”€ 30.2 The Producer/Consumer (Bound Buffer) Problem (16,908 chars)\n",
      "    â”œâ”€ 30.3 Covering Conditions (3,383 chars)\n",
      "    â”œâ”€ 30.4 Summary (2,484 chars)\n",
      "  â€¢ 31 Semaphores (Part II): 377-394 (18p, 36,521 chars, 8 subsections)\n",
      "    â”œâ”€ 31.1 Semaphores: A Deï¬nition (3,326 chars)\n",
      "    â”œâ”€ 31.2 Binary Semaphores (Locks) (3,672 chars)\n",
      "    â”œâ”€ 31.3 Semaphores As Condition Variables (3,968 chars)\n",
      "    â”œâ”€ 31.4 The Producer/Consumer (Bounded-Buffer) Problem (8,195 chars)\n",
      "    â”œâ”€ 31.5 Reader-Writer Locks (4,633 chars)\n",
      "    â”œâ”€ 31.6 The Dining Philosophers (5,341 chars)\n",
      "    â”œâ”€ 31.7 How To Implement Semaphores (1,839 chars)\n",
      "    â”œâ”€ 31.8 Summary (4,418 chars)\n",
      "  â€¢ 32 Common Concurrency Problems (Part II): 395-408 (14p, 27,486 chars, 4 subsections)\n",
      "    â”œâ”€ 32.1 What Types Of Bugs Exist? (1,937 chars)\n",
      "    â”œâ”€ 32.2 Non-Deadlock Bugs (5,797 chars)\n",
      "    â”œâ”€ 32.3 Deadlock Bugs (15,147 chars)\n",
      "    â”œâ”€ 32.4 Summary (3,844 chars)\n",
      "  â€¢ 33 Event-based Concurrency (Advanced) (Part II): 409-418 (10p, 24,519 chars, 9 subsections)\n",
      "    â”œâ”€ 33.1 The Basic Idea: An Event Loop (1,552 chars)\n",
      "    â”œâ”€ 33.2 An Important API: select() (orpoll() ) (3,126 chars)\n",
      "    â”œâ”€ 33.3 Using select() (1,982 chars)\n",
      "    â”œâ”€ 33.4 Why Simpler? No Locks Needed (896 chars)\n",
      "    â”œâ”€ 33.5 A Problem: Blocking System Calls (1,694 chars)\n",
      "    â”œâ”€ 33.6 A Solution: Asynchronous I/O (5,492 chars)\n",
      "    â”œâ”€ 33.7 Another Problem: State Management (2,440 chars)\n",
      "    â”œâ”€ 33.8 What Is Still Difï¬cult With Events (2,031 chars)\n",
      "    â”œâ”€ 33.9 Summary (3,735 chars)\n",
      "  â€¢ 34 Summary Dialogue on Concurrency (Part II): 419-422 (4p, 2,709 chars, 0 subsections)\n",
      "  â€¢ 35 A Dialogue on Persistence (Part III): 423-424 (2p, 1,870 chars, 0 subsections)\n",
      "  â€¢ 36 I/O Devices (Part III): 425-438 (14p, 27,324 chars, 10 subsections)\n",
      "    â”œâ”€ 36.1 System Architecture (1,524 chars)\n",
      "    â”œâ”€ 36.2 A Canonical Device (1,348 chars)\n",
      "    â”œâ”€ 36.3 The Canonical Protocol (2,379 chars)\n",
      "    â”œâ”€ 36.4 Lowering CPU Overhead With Interrupts (4,118 chars)\n",
      "    â”œâ”€ 36.5 More Efï¬cient Data Movement With DMA (1,987 chars)\n",
      "    â”œâ”€ 36.6 Methods Of Device Interaction (2,037 chars)\n",
      "    â”œâ”€ 36.7 Fitting Into The OS: The Device Driver (3,356 chars)\n",
      "    â”œâ”€ 36.8 Case Study: A Simple IDE Disk Driver (4,859 chars)\n",
      "    â”œâ”€ 36.9 Historical Notes (1,694 chars)\n",
      "    â”œâ”€ 36.10 Summary (3,309 chars)\n",
      "  â€¢ 37 Hard Disk Drives (Part III): 439-456 (18p, 35,452 chars, 6 subsections)\n",
      "    â”œâ”€ 37.1 The Interface (1,501 chars)\n",
      "    â”œâ”€ 37.2 Basic Geometry (1,716 chars)\n",
      "    â”œâ”€ 37.3 A Simple Disk Drive (8,333 chars)\n",
      "    â”œâ”€ 37.4 I/O Time: Doing The Math (6,924 chars)\n",
      "    â”œâ”€ 37.5 Disk Scheduling (9,096 chars)\n",
      "    â”œâ”€ 37.6 Summary (6,979 chars)\n",
      "  â€¢ 38 Redundant Arrays of Inexpensive Disks (RAIDs) (Part III): 457-476 (20p, 46,020 chars, 10 subsections)\n",
      "    â”œâ”€ 38.1 Interface And RAID Internals (1,638 chars)\n",
      "    â”œâ”€ 38.2 Fault Model (1,241 chars)\n",
      "    â”œâ”€ 38.3 How To Evaluate A RAID (1,565 chars)\n",
      "    â”œâ”€ 38.4 RAID Level 0: Striping (9,269 chars)\n",
      "    â”œâ”€ 38.5 RAID Level 1: Mirroring (7,508 chars)\n",
      "    â”œâ”€ 38.6 RAID Level 4: Saving Space With Parity (10,204 chars)\n",
      "    â”œâ”€ 38.7 RAID Level 5: Rotating Parity (2,467 chars)\n",
      "    â”œâ”€ 38.8 RAID Comparison: A Summary (1,496 chars)\n",
      "    â”œâ”€ 38.9 Other Interesting RAID Issues (973 chars)\n",
      "    â”œâ”€ 38.10 Summary (5,941 chars)\n",
      "  â€¢ 39 Interlude: File and Directories (Part III): 477-496 (20p, 41,485 chars, 16 subsections)\n",
      "    â”œâ”€ 39.1 Files and Directories (4,071 chars)\n",
      "    â”œâ”€ 39.2 The File System Interface (384 chars)\n",
      "    â”œâ”€ 39.3 Creating Files (1,920 chars)\n",
      "    â”œâ”€ 39.4 Reading and Writing Files (4,854 chars)\n",
      "    â”œâ”€ 39.5 Reading And Writing, But Not Sequentially (2,910 chars)\n",
      "    â”œâ”€ 39.6 Writing Immediately with fsync() (2,246 chars)\n",
      "    â”œâ”€ 39.7 Renaming Files (1,856 chars)\n",
      "    â”œâ”€ 39.8 Getting Information About Files (2,080 chars)\n",
      "    â”œâ”€ 39.9 Removing Files (862 chars)\n",
      "    â”œâ”€ 39.10 Making Directories (2,413 chars)\n",
      "    â”œâ”€ 39.11 Reading Directories (1,613 chars)\n",
      "    â”œâ”€ 39.12 Deleting Directories (498 chars)\n",
      "    â”œâ”€ 39.13 Hard Links (3,886 chars)\n",
      "    â”œâ”€ 39.14 Symbolic Links (2,890 chars)\n",
      "    â”œâ”€ 39.15 Making and Mounting a File System (3,032 chars)\n",
      "    â”œâ”€ 39.16 Summary (4,382 chars)\n",
      "  â€¢ 40 File System Implementation (Part III): 497-514 (18p, 40,804 chars, 8 subsections)\n",
      "    â”œâ”€ 40.1 The Way To Think (1,843 chars)\n",
      "    â”œâ”€ 40.2 Overall Organization (4,975 chars)\n",
      "    â”œâ”€ 40.3 File Organization: The Inode (11,386 chars)\n",
      "    â”œâ”€ 40.4 Directory Organization (2,315 chars)\n",
      "    â”œâ”€ 40.5 Free Space Management (1,972 chars)\n",
      "    â”œâ”€ 40.6 Access Paths: Reading and Writing (8,397 chars)\n",
      "    â”œâ”€ 40.7 Caching and Buffering (3,477 chars)\n",
      "    â”œâ”€ 40.8 Summary (5,044 chars)\n",
      "  â€¢ 41 Locality and The Fast File System (Part III): 515-526 (12p, 24,032 chars, 8 subsections)\n",
      "    â”œâ”€ 41.1 The Problem: Poor Performance (2,869 chars)\n",
      "    â”œâ”€ 41.2 FFS: Disk Awareness Is The Solution (846 chars)\n",
      "    â”œâ”€ 41.3 Organizing Structure: The Cylinder Group (2,834 chars)\n",
      "    â”œâ”€ 41.4 Policies: How To Allocate Files and Directories (2,230 chars)\n",
      "    â”œâ”€ 41.5 Measuring File Locality (2,945 chars)\n",
      "    â”œâ”€ 41.6 The Large-File Exception (4,573 chars)\n",
      "    â”œâ”€ 41.7 A Few Other Things About FFS (5,030 chars)\n",
      "    â”œâ”€ 41.8 Summary (1,690 chars)\n",
      "  â€¢ 42 Crash Consistency: FSCK and Journaling (Part III): 527-546 (20p, 50,549 chars, 5 subsections)\n",
      "    â”œâ”€ 42.1 A Detailed Example (7,138 chars)\n",
      "    â”œâ”€ 42.2 Solution #1: The File System Checker (5,617 chars)\n",
      "    â”œâ”€ 42.3 Solution #2: Journaling (or Write-Ahead Logging) (26,368 chars)\n",
      "    â”œâ”€ 42.4 Solution #3: Other Approaches (3,070 chars)\n",
      "    â”œâ”€ 42.5 Summary (5,688 chars)\n",
      "  â€¢ 43 Log-structured File Systems (Part III): 547-562 (16p, 32,964 chars, 13 subsections)\n",
      "    â”œâ”€ 43.1 Writing To Disk Sequentially (1,707 chars)\n",
      "    â”œâ”€ 43.2 Writing Sequentially And Effectively (2,369 chars)\n",
      "    â”œâ”€ 43.3 How Much To Buffer? (2,165 chars)\n",
      "    â”œâ”€ 43.4 Problem: Finding Inodes (1,271 chars)\n",
      "    â”œâ”€ 43.5 Solution Through Indirection: The Inode Map (2,457 chars)\n",
      "    â”œâ”€ 43.6 The Checkpoint Region (1,417 chars)\n",
      "    â”œâ”€ 43.7 Reading A File From Disk: A Recap (1,043 chars)\n",
      "    â”œâ”€ 43.8 What About Directories? (2,433 chars)\n",
      "    â”œâ”€ 43.9 A New Problem: Garbage Collection (3,929 chars)\n",
      "    â”œâ”€ 43.10 Determining Block Liveness (2,184 chars)\n",
      "    â”œâ”€ 43.11 A Policy Question: Which Blocks To Clean, And When? (1,219 chars)\n",
      "    â”œâ”€ 43.12 Crash Recovery And The Log (2,462 chars)\n",
      "    â”œâ”€ 43.13 Summary (5,211 chars)\n",
      "  â€¢ 44 Data Integrity and Protection (Part III): 563-574 (12p, 29,478 chars, 9 subsections)\n",
      "    â”œâ”€ 44.1 Disk Failure Modes (4,879 chars)\n",
      "    â”œâ”€ 44.2 Handling Latent Sector Errors (2,149 chars)\n",
      "    â”œâ”€ 44.3 Detecting Corruption: The Checksum (7,533 chars)\n",
      "    â”œâ”€ 44.4 Using Checksums (1,306 chars)\n",
      "    â”œâ”€ 44.5 A New Problem: Misdirected Writes (2,513 chars)\n",
      "    â”œâ”€ 44.6 One Last Problem: Lost Writes (1,770 chars)\n",
      "    â”œâ”€ 44.7 Scrubbing (795 chars)\n",
      "    â”œâ”€ 44.8 Overheads Of Checksumming (2,270 chars)\n",
      "    â”œâ”€ 44.9 Summary (5,387 chars)\n",
      "  â€¢ 45 Summary Dialogue on Persistence (Part III): 575-576 (2p, 2,904 chars, 0 subsections)\n",
      "  â€¢ 46 A Dialogue on Distribution (Part III): 577-578 (2p, 2,088 chars, 0 subsections)\n",
      "  â€¢ 47 Distributed Systems (Part III): 579-594 (16p, 36,436 chars, 6 subsections)\n",
      "    â”œâ”€ 47.1 Communication Basics (2,435 chars)\n",
      "    â”œâ”€ 47.2 Unreliable Communication Layers (4,156 chars)\n",
      "    â”œâ”€ 47.3 Reliable Communication Layers (5,456 chars)\n",
      "    â”œâ”€ 47.4 Communication Abstractions (3,769 chars)\n",
      "    â”œâ”€ 47.5 Remote Procedure Call (RPC) (14,356 chars)\n",
      "    â”œâ”€ 47.6 Summary (2,679 chars)\n",
      "  â€¢ 48 Sunâ€™s Network File System (NFS) (Part III): 595-610 (16p, 38,554 chars, 12 subsections)\n",
      "    â”œâ”€ 48.1 A Basic Distributed File System (3,218 chars)\n",
      "    â”œâ”€ 48.2 On To NFS (762 chars)\n",
      "    â”œâ”€ 48.3 Focus: Simple and Fast Server Crash Recovery (765 chars)\n",
      "    â”œâ”€ 48.4 Key To Fast Crash Recovery: Statelessness (3,683 chars)\n",
      "    â”œâ”€ 48.5 The NFSv2 Protocol (4,989 chars)\n",
      "    â”œâ”€ 48.6 From Protocol to Distributed File System (3,447 chars)\n",
      "    â”œâ”€ 48.7 Handling Server Failure with Idempotent Operations (5,110 chars)\n",
      "    â”œâ”€ 48.8 Improving Performance: Client-side Caching (1,606 chars)\n",
      "    â”œâ”€ 48.9 The Cache Consistency Problem (4,435 chars)\n",
      "    â”œâ”€ 48.10 Assessing NFS Cache Consistency (1,186 chars)\n",
      "    â”œâ”€ 48.11 Implications on Server-Side Write Buffering (4,005 chars)\n",
      "    â”œâ”€ 48.12 Summary (3,809 chars)\n",
      "  â€¢ 49 The Andrew File System (AFS) (Part III): 611-624 (14p, 31,580 chars, 9 subsections)\n",
      "    â”œâ”€ 49.1 AFS Version 1 (3,269 chars)\n",
      "    â”œâ”€ 49.2 Problems with Version 1 (3,037 chars)\n",
      "    â”œâ”€ 49.3 Improving the Protocol (815 chars)\n",
      "    â”œâ”€ 49.4 AFS Version 2 (4,732 chars)\n",
      "    â”œâ”€ 49.5 Cache Consistency (4,101 chars)\n",
      "    â”œâ”€ 49.6 Crash Recovery (1,983 chars)\n",
      "    â”œâ”€ 49.7 Scale And Performance Of AFSv2 (5,913 chars)\n",
      "    â”œâ”€ 49.8 AFS: Other Improvements (2,916 chars)\n",
      "    â”œâ”€ 49.9 Summary (3,451 chars)\n",
      "  â€¢ 50 Summary Dialogue on Distribution (Part III): 625-643 (19p, 27,533 chars, 0 subsections)\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ìš”ì•½\n",
    "total_subsections = sum(len(cr.get('subsections', [])) for cr in chapter_ranges)\n",
    "\n",
    "print(\"\\nChapter information:\")\n",
    "for info in chapter_ranges:\n",
    "    part_str = f\"Part {info['part_info']['number']}\" if info['part_info'] else 'Unclassified'\n",
    "    print(f\"  â€¢ {info['toc_entry']['full_title']} ({part_str}): {info['start_page']}-{info['end_page']} \"\n",
    "        f\"({info['page_count']}p, {len(info['text']):,} chars, {len(info.get('subsections', []))} subsections)\")\n",
    "    \n",
    "    # ì„œë¸Œì„¹ì…˜ ì •ë³´ ì¶œë ¥\n",
    "    if info.get('subsections'):\n",
    "        for subsection in info['subsections']:\n",
    "            print(f\"    â”œâ”€ {subsection['toc_entry']['number']} {subsection['toc_entry']['title']} ({len(subsection['text']):,} chars)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
