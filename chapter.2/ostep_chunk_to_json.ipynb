{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332cb09",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: OSTEP 전처리 및 청크 JSON 저장\n",
    "\n",
    "이 노트북은 OSTEP 교재 PDF를 전처리하고 토큰 기반 청킹 후 JSON으로 저장하는 전체 과정을 단계별로 실습합니다.\n",
    "\n",
    "## 📚 학습 목표\n",
    "- PDF → 텍스트 추출 파이프라인 구성 및 품질 관리 포인트 이해\n",
    "- 목차(챕터/파트/서브섹션) 파싱과 페이지 범위 계산 방법 습득\n",
    "- 문장 경계 유지형 토큰 기반 청킹과 오버랩 설계 이해\n",
    "\n",
    "## 📋 실습 구성\n",
    "- 1️⃣ 환경 설정: 패키지 설치, 경로/상수 정의, 데이터 위치 확인\n",
    "- 2️⃣ 임포트/상수: 정규식 패턴, 토큰/오버랩, 분리 규칙 등 설정\n",
    "- 3️⃣ PDF 로드/기본 함수: 페이지 안전 추출 유틸리티 준비\n",
    "- 4️⃣ 목차 파싱: 챕터/파트/서브섹션 구조화\n",
    "- 5️⃣ 범위 계산/텍스트 추출: 챕터별 범위 산출 및 정제\n",
    "- 6️⃣ 청크 생성/저장: 문장 경계 유지형 토큰 청킹 → JSON 저장\n",
    "- 7️⃣ 결과 요약: 범위/문자수/서브섹션 통계 출력\n",
    "\n",
    "> ⚠️ 실습 셀 실행 전, 환경 설정 셀(1️⃣)을 먼저 실행하고 OSTEP PDF 경로를 확인하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc101372",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1️⃣ Google Colab 환경 설정\n",
    "\n",
    "이 노트북은 **Google Colab에서 GPU를 사용**하여 실행하도록 설계되었습니다.\n",
    "\n",
    "### 📌 실행 전 준비사항\n",
    "1. **런타임 유형 설정**: 메뉴에서 `런타임` → `런타임 유형 변경` → `GPU` 선택\n",
    "2. **첫 번째 코드 셀 실행**: Google Drive 마운트 및 필수 패키지 자동 설치\n",
    "3. **OSTEP PDF 업로드**: 최초 1회만 `/content/drive/MyDrive/ostep_rag/data/documents/ostep.pdf` 위치에 업로드\n",
    "\n",
    "> ⚠️ **중요**: 아래 코드 셀을 가장 먼저 실행하여 환경을 설정하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Google Colab 환경 설정\n",
    "# ========================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive 마운트\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 필요 패키지 설치\n",
    "!pip -q install PyPDF2 pdfplumber pymupdf tiktoken\n",
    "\n",
    "# 경로 설정 (Colab 전용)\n",
    "BASE_DIR = \"/content/drive/MyDrive/ostep_rag\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "DOC_ID = \"ostep\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"chunk\")\n",
    "PDF_PATH = os.path.join(DATA_DIR, \"documents\", \"ostep.pdf\")\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(os.path.join(DATA_DIR, \"documents\"), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# OSTEP PDF 업로드 확인\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"⚠️ 구글 드라이브에 OSTEP PDF를 업로드하세요:\")\n",
    "    print(f\"   {PDF_PATH}\")\n",
    "    print(f\"\\n📁 좌측 파일 탭 → drive → MyDrive → ostep_rag → data → documents 폴더에 ostep.pdf 업로드\")\n",
    "else:\n",
    "    print(f\"✅ OSTEP PDF 파일 확인됨: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14830a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2️⃣ 임포트 및 설정 상수\n",
    "\n",
    "이 셀에서는 필요한 라이브러리를 임포트하고 전역 설정 상수를 정의합니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- PyPDF2를 사용하여 PDF 파일 처리\n",
    "- 정규식 패턴을 사용하여 목차에서 챕터, 파트, 서브섹션 파싱\n",
    "- 토큰 기반 청킹 설정(최대 토큰, 오버랩 토큰)\n",
    "- 출력 디렉토리 및 문서 ID 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba277ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 설정 상수\n",
    "TOC_START_PAGE = 15\n",
    "TOC_END_PAGE = 25\n",
    "PAGE_OFFSET = 36\n",
    "CHUNK_MAX_TOKENS = 400\n",
    "CHUNK_OVERLAP_TOKENS = 80\n",
    "SENTENCE_SPLIT_REGEX = r'(?<=[.!?])\\s+(?=[A-Z0-9])'\n",
    "\n",
    "# 정규식 패턴\n",
    "MAIN_CHAPTER_PATTERNS = [\n",
    "    r'^(\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "PART_PATTERNS = [\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "SUBSECTION_PATTERNS = [\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b647b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3️⃣ PDF 로드 및 기본 함수\n",
    "\n",
    "이 셀에서는 PDF 파일을 로드하고 기본적인 텍스트 추출 함수를 정의합니다.\n",
    "\n",
    "**주요 함수:**\n",
    "- `load_pdf()`: PDF 파일을 로드하고 페이지 수를 확인\n",
    "- `extract_pdf_text()`: 지정된 페이지 범위에서 텍스트를 추출\n",
    "\n",
    "**실행 결과:**\n",
    "- PDF 파일이 성공적으로 로드되고 전체 페이지 수가 출력됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053adb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path: str):\n",
    "    \"\"\"PDF 파일 로드\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF 파일을 찾을 수 없습니다: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    print(f\"PDF 로드 완료: {len(reader.pages)} 페이지\")\n",
    "    return reader\n",
    "\n",
    "def extract_pdf_text(reader, start_page: int, end_page: int):\n",
    "    \"\"\"PDF 페이지 범위에서 텍스트 추출\"\"\"\n",
    "    total_pages = len(reader.pages)\n",
    "    if start_page < 1 or end_page > total_pages:\n",
    "        start_page = max(1, start_page)\n",
    "        end_page = min(total_pages, end_page)\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num in range(start_page - 1, end_page):\n",
    "        page = reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# PDF 로드 실행\n",
    "pdf_path = PDF_PATH\n",
    "reader = load_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcc01",
   "metadata": {},
   "source": [
    "---\n",
    "## 4️⃣ 목차 파싱\n",
    "\n",
    "이 셀에서는 PDF 목차를 파싱하여 챕터, 파트, 서브섹션 정보를 추출합니다.\n",
    "\n",
    "**주요 함수:**\n",
    "- `create_toc_entry()`: 목차 항목을 딕셔너리로 생성\n",
    "- `parse_chapter_line()`, `parse_part_line()`, `parse_subsection_line()`: 각 유형별 라인 파싱\n",
    "- `parse_toc()`: 목차 전체를 파싱하여 3가지 유형으로 분류\n",
    "\n",
    "**실행 결과:**\n",
    "- 챕터, 파트, 서브섹션의 개수가 출력됩니다.\n",
    "- 정규식 패턴으로 목차의 구조를 인식하여 계층 구조를 파악합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toc_entry(entry_type: str, number, title: str, toc_page: int):\n",
    "    \"\"\"TOC 항목 생성 (딕셔너리 반환)\"\"\"\n",
    "    actual_page = toc_page + PAGE_OFFSET\n",
    "    \n",
    "    if entry_type == 'part':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"part_{number}\"\n",
    "    elif entry_type == 'subsection':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"subsec_{number}\"\n",
    "    else:  # chapter\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"ch_{number}\"\n",
    "    \n",
    "    return {\n",
    "        'entry_type': entry_type,\n",
    "        'number': number,\n",
    "        'title': title,\n",
    "        'toc_page': toc_page,\n",
    "        'actual_page': actual_page,\n",
    "        'full_title': full_title,\n",
    "        'id': entry_id\n",
    "    }\n",
    "\n",
    "def clean_toc_title(title: str):\n",
    "    \"\"\"목차 제목 정제\"\"\"\n",
    "    title = re.sub(r'\\s*[\\.\\s]{4,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.{3,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.+\\s*$', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    return title.strip()\n",
    "\n",
    "def is_valid_title(title: str):\n",
    "    \"\"\"제목 유효성 검사\"\"\"\n",
    "    return (bool(title) and \n",
    "            len(title) > 3 and \n",
    "            not title.strip().isdigit() and \n",
    "            title[0].isupper())\n",
    "\n",
    "def parse_chapter_line(line: str):\n",
    "    \"\"\"챕터 라인 파싱\"\"\"\n",
    "    for pattern in MAIN_CHAPTER_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            chapter_num = int(match.group(1))\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('chapter', chapter_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_part_line(line: str):\n",
    "    \"\"\"파트 라인 파싱\"\"\"\n",
    "    for pattern in PART_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            part_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('part', part_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_subsection_line(line: str):\n",
    "    \"\"\"서브섹션 라인 파싱 (예: 2.1, 4.2 등)\"\"\"\n",
    "    for pattern in SUBSECTION_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            subsection_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('subsection', subsection_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_toc(reader):\n",
    "    \"\"\"목차 파싱 (챕터 + 파트 + 서브섹션)\"\"\"\n",
    "    print(f\"목차 파싱 중 (페이지 {TOC_START_PAGE}-{TOC_END_PAGE})...\")\n",
    "    \n",
    "    toc_text = extract_pdf_text(reader, TOC_START_PAGE, TOC_END_PAGE)\n",
    "    if not toc_text:\n",
    "        print(\"목차 텍스트 추출 실패\")\n",
    "        return [], [], []\n",
    "    \n",
    "    chapters = []\n",
    "    parts = []\n",
    "    subsections = []\n",
    "    \n",
    "    for line in toc_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # 파트 파싱 우선 시도\n",
    "        part_entry = parse_part_line(line)\n",
    "        if part_entry:\n",
    "            parts.append(part_entry)\n",
    "            continue\n",
    "        \n",
    "        # 서브섹션 파싱 시도\n",
    "        subsection_entry = parse_subsection_line(line)\n",
    "        if subsection_entry:\n",
    "            subsections.append(subsection_entry)\n",
    "            continue\n",
    "        \n",
    "        # 챕터 파싱\n",
    "        chapter_entry = parse_chapter_line(line)\n",
    "        if chapter_entry:\n",
    "            chapters.append(chapter_entry)\n",
    "    \n",
    "    # 페이지 순서로 정렬\n",
    "    chapters.sort(key=lambda x: x['toc_page'])\n",
    "    parts.sort(key=lambda x: x['toc_page'])\n",
    "    subsections.sort(key=lambda x: x['toc_page'])\n",
    "    \n",
    "    print(f\"목차 파싱 완료: {len(chapters)}개 챕터, {len(parts)}개 파트, {len(subsections)}개 서브섹션\")\n",
    "    return chapters, parts, subsections\n",
    "\n",
    "# 목차 파싱 실행\n",
    "chapters, parts, subsections = parse_toc(reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee5e56",
   "metadata": {},
   "source": [
    "---\n",
    "## 5️⃣ 챕터 범위 계산\n",
    "\n",
    "이 셀에서는 각 챕터가 차지하는 페이지 범위를 계산합니다.\n",
    "\n",
    "**주요 함수:**\n",
    "- `find_part_for_chapter()`: 챕터가 속한 파트를 찾기\n",
    "- `calculate_chapter_ranges()`: 챕터별 시작/종료 페이지를 계산\n",
    "\n",
    "**실행 결과:**\n",
    "- 각 챕터의 페이지 범위가 계산됩니다.\n",
    "- 다음 챕터 시작 전까지가 현재 챕터의 마지막 페이지입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_part_for_chapter(chapter, parts):\n",
    "    \"\"\"챕터가 속한 파트 찾기\"\"\"\n",
    "    current_part = None\n",
    "    for part in parts:\n",
    "        if part['actual_page'] <= chapter['actual_page']:\n",
    "            current_part = part\n",
    "        else:\n",
    "            break\n",
    "    return current_part\n",
    "\n",
    "def calculate_chapter_ranges(reader, chapters, parts):\n",
    "    \"\"\"챕터별 페이지 범위 계산\"\"\"\n",
    "    print(\"챕터별 페이지 범위 계산 중...\")\n",
    "    \n",
    "    if not chapters:\n",
    "        print(\"메인 챕터를 찾을 수 없습니다.\")\n",
    "        return []\n",
    "    \n",
    "    chapter_ranges = []\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    for i, chapter in enumerate(chapters):\n",
    "        start_page = chapter['actual_page']\n",
    "        \n",
    "        # 다음 챕터가 있으면 그 전 페이지까지, 없으면 PDF 끝까지\n",
    "        if i + 1 < len(chapters):\n",
    "            end_page = chapters[i + 1]['actual_page'] - 1\n",
    "        else:\n",
    "            end_page = total_pages\n",
    "        \n",
    "        # 해당 챕터가 속한 파트 찾기\n",
    "        part_info = find_part_for_chapter(chapter, parts)\n",
    "        \n",
    "        # 페이지 범위 유효성 검사\n",
    "        if start_page <= end_page:\n",
    "            chapter_range = {\n",
    "                'toc_entry': chapter,\n",
    "                'part_info': part_info,\n",
    "                'start_page': start_page,\n",
    "                'end_page': end_page,\n",
    "                'page_count': end_page - start_page + 1\n",
    "            }\n",
    "            chapter_ranges.append(chapter_range)\n",
    "    \n",
    "    print(f\"챕터 범위 계산 완료: {len(chapter_ranges)}개 챕터\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# 챕터 범위 계산 실행\n",
    "chapter_ranges = calculate_chapter_ranges(reader, chapters, parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f526e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6️⃣ 텍스트 추출 및 서브섹션 분할\n",
    "\n",
    "이 셀에서는 각 챕터의 텍스트를 추출하고 서브섹션으로 분할합니다.\n",
    "\n",
    "**주요 함수:**\n",
    "- `clean_text()`: PDF 헤더/푸터, 페이지 번호 제거 및 공백 정리\n",
    "- `find_chapter_subsections()`: 챕터에 속한 서브섹션들 찾기\n",
    "- `split_text_by_subsections()`: 텍스트를 서브섹션별로 분할\n",
    "- `extract_texts()`: 모든 챕터에서 텍스트 추출 및 서브섹션 분할\n",
    "\n",
    "**실행 결과:**\n",
    "- 각 챕터의 텍스트가 추출되고 서브섹션별로 분할됩니다.\n",
    "- 헤더/푸터 등 불필요한 요소가 제거된 깨끗한 텍스트를 얻습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"텍스트 정제 및 헤더/푸터 제거\"\"\"\n",
    "    # PDF 헤더/푸터 제거\n",
    "    header_pattern = r'c/circle\\s*copyrt\\s*\\d+,?\\s*A\\s+RPACI\\s*-?\\s*D\\s*USSEAU\\s*THREE\\s+EASY\\s+PIECES'\n",
    "    text = re.sub(header_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    footer_pattern = r'OPERATING\\s+SYSTEMS\\s+\\[VERSION\\s+[\\d.]+\\]\\s+WWW\\s*\\.\\s*OSTEP\\s*\\.\\s*ORG.*$'\n",
    "    text = re.sub(footer_pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # 페이지 번호 패턴 제거\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 공백 정리\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_chapter_subsections(chapter, subsections):\n",
    "    \"\"\"챕터에 속한 서브섹션들 찾기\"\"\"\n",
    "    chapter_num = str(chapter['number'])\n",
    "    chapter_subsections = []\n",
    "    \n",
    "    for subsection in subsections:\n",
    "        # 서브섹션 번호가 해당 챕터로 시작하는지 확인 (예: 2.1, 2.2는 챕터 2에 속함)\n",
    "        subsection_num = str(subsection['number'])\n",
    "        if subsection_num.startswith(f\"{chapter_num}.\"):\n",
    "            chapter_subsections.append(subsection)\n",
    "    \n",
    "    # 번호 순으로 정렬\n",
    "    chapter_subsections.sort(key=lambda x: float(x['number']))\n",
    "    return chapter_subsections\n",
    "\n",
    "def build_flexible_title_pattern(title: str):\n",
    "    \"\"\"제목을 안전한 정규식으로 변환\"\"\"\n",
    "    parts = []\n",
    "    for ch in title:\n",
    "        if ch.isspace():\n",
    "            parts.append(r\"\\s+\")\n",
    "        elif ch == '(':\n",
    "            parts.append(r\"\\s*\\(\\s*\")\n",
    "        elif ch == ')':\n",
    "            parts.append(r\"\\s*\\)\\s*\")\n",
    "        else:\n",
    "            parts.append(re.escape(ch))\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_text_by_subsections(chapter_text: str, chapter, chapter_subsections):\n",
    "    \"\"\"챕터 텍스트를 서브섹션별로 분할\"\"\"\n",
    "    if not chapter_subsections:\n",
    "        return []\n",
    "    \n",
    "    subsection_data_list = []\n",
    "    \n",
    "    split_points = []\n",
    "    \n",
    "    for subsection in chapter_subsections:\n",
    "        section_title = f\"{subsection['number']} {subsection['title']}\"\n",
    "        \n",
    "        patterns_to_try = [\n",
    "            re.escape(section_title),\n",
    "            f\"{re.escape(str(subsection['number']))}\\\\s+{build_flexible_title_pattern(subsection['title'])}\"\n",
    "        ]\n",
    "\n",
    "        # \"The\" 시작하는 제목의 특수 처리\n",
    "        lower_title = subsection['title'].lower()\n",
    "        if lower_title.startswith(\"the \"):\n",
    "            after_the = subsection['title'][4:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        elif lower_title.startswith(\"the\"):\n",
    "            after_the = subsection['title'][3:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        \n",
    "        found = False\n",
    "        for pattern in patterns_to_try:\n",
    "            matches = list(re.finditer(pattern, chapter_text, re.IGNORECASE | re.MULTILINE))\n",
    "            if matches:\n",
    "                # 첫 번째 매치 사용\n",
    "                match = matches[0]\n",
    "                split_points.append({\n",
    "                    'subsection': subsection,\n",
    "                    'start_pos': match.start(),\n",
    "                    'end_pos': match.end(),\n",
    "                    'title_match': match.group().strip()\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"  서브섹션 '{subsection['number']} {subsection['title']}' 제목을 텍스트에서 찾을 수 없습니다.\")\n",
    "    \n",
    "    # 위치별로 정렬\n",
    "    split_points.sort(key=lambda x: x['start_pos'])\n",
    "    \n",
    "    # 텍스트 분할\n",
    "    for i, split_point in enumerate(split_points):\n",
    "        start_pos = split_point['start_pos']\n",
    "        \n",
    "        # 다음 서브섹션의 시작 위치 또는 텍스트 끝\n",
    "        if i + 1 < len(split_points):\n",
    "            end_pos = split_points[i + 1]['start_pos']\n",
    "        else:\n",
    "            end_pos = len(chapter_text)\n",
    "        \n",
    "        # 서브섹션 텍스트 추출\n",
    "        subsection_text = chapter_text[start_pos:end_pos].strip()\n",
    "        \n",
    "        if subsection_text:\n",
    "            subsection_data = {\n",
    "                'toc_entry': split_point['subsection'],\n",
    "                'parent_chapter': chapter,\n",
    "                'text': clean_text(subsection_text)\n",
    "            }\n",
    "            subsection_data_list.append(subsection_data)\n",
    "    \n",
    "    return subsection_data_list\n",
    "\n",
    "def extract_texts(reader, chapter_ranges, subsections):\n",
    "    \"\"\"챕터별 텍스트 추출 및 서브섹션 분할\"\"\"\n",
    "    print(f\"챕터별 텍스트 추출 및 서브섹션 분할 중...\")\n",
    "    for chapter_range in chapter_ranges:\n",
    "        chapter = chapter_range['toc_entry']\n",
    "        \n",
    "        # 텍스트 추출\n",
    "        text = extract_pdf_text(reader, chapter_range['start_page'], chapter_range['end_page'])\n",
    "        chapter_range['text'] = clean_text(text)\n",
    "        \n",
    "        # 해당 챕터의 서브섹션들 찾기\n",
    "        chapter_subsections = find_chapter_subsections(chapter, subsections)\n",
    "        \n",
    "        if chapter_subsections:\n",
    "            # 서브섹션별로 텍스트 분할\n",
    "            subsection_data_list = split_text_by_subsections(chapter_range['text'], chapter, chapter_subsections)\n",
    "            chapter_range['subsections'] = subsection_data_list\n",
    "        else:\n",
    "            chapter_range['subsections'] = []\n",
    "    \n",
    "    print(f\"챕터별 텍스트 추출 및 서브섹션 분할 완료: {len(chapter_ranges)}개 챕터\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# 텍스트 추출 및 서브섹션 분할 실행\n",
    "chapter_ranges = extract_texts(reader, chapter_ranges, subsections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b194f",
   "metadata": {},
   "source": [
    "---\n",
    "## 7️⃣ 청크 생성 및 JSON 저장\n",
    "\n",
    "이 셀에서는 텍스트를 토큰 기반으로 청킹하고 JSON 파일로 저장합니다.\n",
    "\n",
    "**주요 함수:**\n",
    "- `estimate_tokens_length()`: tiktoken 또는 근사치로 토큰 수 추정\n",
    "- `split_text_into_sentences()`: 정규식으로 문장 분리\n",
    "- `chunk_sentences_hybrid()`: 문장 경계를 유지하며 토큰 제한으로 청킹\n",
    "- `build_all_chunk_records()`: 모든 청크를 레코드로 생성\n",
    "- `write_json()`: JSON 파일로 저장\n",
    "\n",
    "**실행 결과:**\n",
    "- 문장 경계를 유지하면서 토큰 제한(400토큰, 20% 오버랩)에 맞춰 청킹됩니다.\n",
    "- JSON 파일로 저장되어 RAG 시스템에서 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_output_dir(output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def build_output_path(output_dir: str, max_tokens: int, overlap_tokens: int):\n",
    "    overlap_pct = int(round((overlap_tokens / max_tokens) * 100)) if max_tokens > 0 else 0\n",
    "    filename = f\"{DOC_ID}_tok{max_tokens}_ov{overlap_pct}.json\"\n",
    "    return str(Path(output_dir) / filename)\n",
    "\n",
    "def try_import_tiktoken():\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def estimate_tokens_length(text: str):\n",
    "    \"\"\"토큰 길이 추정 (tiktoken 우선, 없으면 근사치)\"\"\"\n",
    "    tiktoken = try_import_tiktoken()\n",
    "    if tiktoken is not None:\n",
    "        try:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except Exception:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "    return int(len(text.split()) * 1.3)\n",
    "\n",
    "def split_text_into_sentences(text: str):\n",
    "    \"\"\"정규식 기반 문장 분리\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    sentences = re.split(SENTENCE_SPLIT_REGEX, text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def chunk_sentences_hybrid(sentences, max_tokens: int, overlap_tokens: int):\n",
    "    \"\"\"문장 경계를 유지하며 토큰 상한으로 청킹\"\"\"\n",
    "    chunks = []\n",
    "    if not sentences:\n",
    "        return chunks\n",
    "\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    for sent in sentences:\n",
    "        sent_tokens = estimate_tokens_length(sent)\n",
    "        \n",
    "        if sent_tokens > max_tokens:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "                current = []\n",
    "                current_tokens = 0\n",
    "            chunks.append(sent.strip())\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sent_tokens <= max_tokens:\n",
    "            current.append(sent)\n",
    "            current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "            \n",
    "            if overlap_tokens > 0:\n",
    "                overlap_bucket = []\n",
    "                overlap_count = 0\n",
    "                for prev_sent in reversed(current):\n",
    "                    t = estimate_tokens_length(prev_sent)\n",
    "                    if overlap_count + t > overlap_tokens:\n",
    "                        break\n",
    "                    overlap_bucket.append(prev_sent)\n",
    "                    overlap_count += t\n",
    "                overlap_bucket.reverse()\n",
    "                current = overlap_bucket + [sent]\n",
    "                current_tokens = sum(estimate_tokens_length(s) for s in current)\n",
    "            else:\n",
    "                current = [sent]\n",
    "                current_tokens = sent_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current).strip())\n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(chapter_id: str, subsec_id, chunk_index: int):\n",
    "    base = f\"{chapter_id}\"\n",
    "    if subsec_id:\n",
    "        base += f\"__{subsec_id}\"\n",
    "    return f\"{base}__{chunk_index:04d}\"\n",
    "\n",
    "def find_part_for_page(page: int, part_lookup_by_page):\n",
    "\tcurrent = None\n",
    "\tfor start_page, part in part_lookup_by_page:\n",
    "\t\tif start_page <= page:\n",
    "\t\t\tcurrent = part\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\treturn current\n",
    "\n",
    "def build_all_chunk_records(pdf_path: str, parts, chapter_ranges, max_tokens: int, overlap_tokens: int):\n",
    "    part_lookup_by_page = []\n",
    "    for p in parts:\n",
    "        part_lookup_by_page.append((p['actual_page'], p))\n",
    "    part_lookup_by_page.sort(key=lambda x: x[0])\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for cr in chapter_ranges:\n",
    "        chapter = cr['toc_entry']\n",
    "        chapter_id = chapter['id']\n",
    "        chapter_title = chapter['title']\n",
    "        part_info = cr.get('part_info')\n",
    "        if part_info is None:\n",
    "            part_info = find_part_for_page(cr['start_page'], part_lookup_by_page)\n",
    "\n",
    "        # 서브섹션이 있으면 서브섹션 기준, 없으면 챕터 전체 텍스트 기준으로 청킹\n",
    "        if cr.get('subsections'):\n",
    "            for sub in cr['subsections']:\n",
    "                subsec = sub\n",
    "                sentences = split_text_into_sentences(subsec['text'])\n",
    "                chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "                for idx, chunk_text in enumerate(chunks):\n",
    "                    records.append({\n",
    "                        'chunk_id': make_chunk_id(chapter_id, subsec['toc_entry']['id'], idx),\n",
    "                        'chapter_id': chapter_id,\n",
    "                        'chapter_title': chapter_title,\n",
    "                        'subsection_id': subsec['toc_entry']['id'],\n",
    "                        'subsection_title': f\"{subsec['toc_entry']['number']} {subsec['toc_entry']['title']}\",\n",
    "                        'text': chunk_text,\n",
    "                    })\n",
    "        else:\n",
    "            sentences = split_text_into_sentences(cr['text'])\n",
    "            chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                records.append({\n",
    "                    'chunk_id': make_chunk_id(chapter_id, None, idx),\n",
    "                    'chapter_id': chapter_id,\n",
    "                    'chapter_title': chapter_title,\n",
    "                    'subsection_id': None,\n",
    "                    'subsection_title': None,\n",
    "                    'text': chunk_text,\n",
    "                })\n",
    "\n",
    "    return records\n",
    "\n",
    "def write_json(path: str, records):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 청크 생성 및 저장\n",
    "print(\"\\n5단계: 청크 생성 및 저장(JSON)\")\n",
    "print(\"-\" * 40)\n",
    "ensure_output_dir(OUTPUT_DIR)\n",
    "output_path = build_output_path(OUTPUT_DIR, CHUNK_MAX_TOKENS, CHUNK_OVERLAP_TOKENS)\n",
    "all_chunk_records = build_all_chunk_records(\n",
    "\tpdf_path=pdf_path,\n",
    "\tparts=parts,\n",
    "\tchapter_ranges=chapter_ranges,\n",
    "\tmax_tokens=CHUNK_MAX_TOKENS,\n",
    "\toverlap_tokens=CHUNK_OVERLAP_TOKENS,\n",
    ")\n",
    "write_json(output_path, all_chunk_records)\n",
    "print(f\"청크 저장 완료: {output_path} ({len(all_chunk_records)} chunks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9d4bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 8️⃣ 결과 요약 출력\n",
    "\n",
    "이 셀에서는 전처리된 데이터의 요약 정보를 출력합니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- 챕터별 페이지 범위, 문자 수, 서브섹션 개수\n",
    "- 서브섹션별 문자 수\n",
    "\n",
    "**실행 결과:**\n",
    "- 각 챕터와 서브섹션의 상세 정보가 출력됩니다.\n",
    "- 전처리 품질을 확인하고 개선점을 파악할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 요약\n",
    "total_subsections = sum(len(cr.get('subsections', [])) for cr in chapter_ranges)\n",
    "\n",
    "print(\"\\nChapter information:\")\n",
    "for info in chapter_ranges:\n",
    "    part_str = f\"Part {info['part_info']['number']}\" if info['part_info'] else 'Unclassified'\n",
    "    print(f\"  • {info['toc_entry']['full_title']} ({part_str}): {info['start_page']}-{info['end_page']} \"\n",
    "        f\"({info['page_count']}p, {len(info['text']):,} chars, {len(info.get('subsections', []))} subsections)\")\n",
    "    \n",
    "    # 서브섹션 정보 출력\n",
    "    if info.get('subsections'):\n",
    "        for subsection in info['subsections']:\n",
    "            print(f\"    ├─ {subsection['toc_entry']['number']} {subsection['toc_entry']['title']} ({len(subsection['text']):,} chars)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
