{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332cb09",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: ë°ì´í„° ì¤€ë¹„ - OSTEP êµì¬ ì „ì²˜ë¦¬ ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Chapter 2: ë°ì´í„° ì¤€ë¹„**ì˜ ë‚´ìš©ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "- OSTEP êµì¬ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ RAG í™œìš© ë°©ì•ˆ ì´í•´\n",
    "- PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ê¸°ë²• ìŠµë“\n",
    "- í…ìŠ¤íŠ¸ êµ¬ì¡°í™” ë° ê³„ì¸µ ì •ë³´ ë³´ì¡´ ë°©ë²• í•™ìŠµ\n",
    "- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "- 1ï¸âƒ£ í™˜ê²½ ì„¤ì •: í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸\n",
    "- 2ï¸âƒ£ êµì¬ ë°ì´í„° ì„¤ëª…: OSTEP êµì¬ ì†Œê°œ ë° RAG í™œìš© ë°©ì•ˆ\n",
    "- 3ï¸âƒ£ ë¬¸ì„œ íŒŒì‹± ë° ì „ì²˜ë¦¬: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°\n",
    "- 4ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ ë° êµ¬ì¡°í™”: ê³„ì¸µ êµ¬ì¡° íŒŒì‹± ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "- 5ï¸âƒ£ PDF ë³¸ë¬¸ ì¶”ì¶œ ì‹¤ìŠµ: ì „ì²˜ë¦¬ í•¨ìˆ˜ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸\n",
    "\n",
    "> âš ï¸ *ì‹¤ìŠµ ì…€ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ë°˜ë“œì‹œ í™˜ê²½ ì„¤ì • ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc101372",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1ï¸âƒ£ í™˜ê²½ ì„¤ì • (Environment Setup)\n",
    "\n",
    "RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤. PDF ì²˜ë¦¬, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ë²¡í„° ì„ë² ë”© ë“±ì„ ìœ„í•œ ë‹¤ì–‘í•œ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229af332",
   "metadata": {},
   "source": [
    "### 1. íŒŒì´ì¬ ê°€ìƒí™˜ê²½ ìƒì„±\n",
    "\n",
    "ë…ë¦½ì ì¸ ê°œë°œ í™˜ê²½ì„ ìœ„í•´ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "$ python -m venv rag_env\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134bb69",
   "metadata": {},
   "source": [
    "### 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì»¤ë„ ì„¤ì •\n",
    "\n",
    "PDF ì²˜ë¦¬, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ë²¡í„° ì„ë² ë”©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "$ ./rag_env/bin/pip install -U pip\n",
    "$ ./rag_env/bin/pip install jupyter ipykernel numpy pandas\n",
    "$ ./rag_env/bin/pip install PyPDF2 pdfplumber pymupdf  # PDF ì²˜ë¦¬\n",
    "$ ./rag_env/bin/pip install faiss-cpu langchain langchain-community  # ë²¡í„° DB ë° RAG\n",
    "$ ./rag_env/bin/pip install tiktoken openai sentence-transformers  # ì„ë² ë”©\n",
    "$ ./rag_env/bin/python -m ipykernel install --user --name=rag-env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74440a4",
   "metadata": {},
   "source": [
    "### 3. ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì»¤ë„ ë³€ê²½\n",
    "\n",
    "ì„¤ì¹˜ ì™„ë£Œ í›„ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì»¤ë„ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **Jupyter Notebook ì‚¬ìš© ì‹œ**: ìƒë‹¨ ë©”ë‰´ì—ì„œ `Kernel` â†’ `Change kernel` â†’ `rag-env` ì„ íƒ\n",
    "2. **JupyterLab ì‚¬ìš© ì‹œ**: ìš°ìƒë‹¨ ì»¤ë„ ì´ë¦„ í´ë¦­ â†’ `rag-env` ì„ íƒ  \n",
    "3. **VS Code ì‚¬ìš© ì‹œ**: ìš°í•˜ë‹¨ ì»¤ë„ ì„ íƒ ë²„íŠ¼ í´ë¦­ â†’ `rag-env` ì„ íƒ\n",
    "\n",
    "> ğŸ’¡ **í™•ì¸ ë°©ë²•**: ì…€ì—ì„œ `import sys; print(sys.executable)` ì‹¤í–‰í•˜ì—¬ ê°€ìƒí™˜ê²½ ê²½ë¡œê°€ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982701b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ êµì¬ ë°ì´í„° ì„¤ëª…: RAGì— í™œìš©í•  ìš´ì˜ì²´ì œ êµì¬ OSTEP\n",
    "\n",
    "OSTEP(Operating Systems: Three Easy Pieces)ëŠ” ìš´ì˜ì²´ì œì˜ í•µì‹¬ ê°œë…ì„ ë‹¤ë£¨ëŠ” ë¬´ë£Œ êµì¬ë¡œ, ê°€ìƒí™”(Virtualization), ë™ì‹œì„±(Concurrency), ì˜ì†ì„±(Persistence)ì´ë¼ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ êµì¬ëŠ” ëª…í™•í•œ ì„¤ëª…ê³¼ ì‹¤ìš©ì ì¸ ì˜ˆì œë¡œ ìœ ëª…í•˜ë©°, ì „ ì„¸ê³„ ë§ì€ ëŒ€í•™ì—ì„œ ìš´ì˜ì²´ì œ ìˆ˜ì—…ì˜ êµì¬ë¡œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì— OSTEPë¥¼ í™œìš©í•˜ëŠ” ì´ìœ ëŠ” êµì¬ì˜ ì²´ê³„ì ì¸ êµ¬ì¡°ì™€ í’ë¶€í•œ ë‚´ìš© ë•Œë¬¸ì…ë‹ˆë‹¤. ê° ì±•í„°ëŠ” ëª…í™•í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ë©°, ê°œë… ì„¤ëª…, ì½”ë“œ ì˜ˆì œ, ì—°ìŠµë¬¸ì œ ë“±ì´ ì˜ ì •ë¦¬ë˜ì–´ ìˆì–´ ì§€ì‹ ë² ì´ìŠ¤ë¡œ í™œìš©í•˜ê¸°ì— ì í•©í•©ë‹ˆë‹¤. PDF í˜•íƒœë¡œ ì œê³µë˜ëŠ” OSTEP êµì¬ë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ RAG ì‹œìŠ¤í…œì˜ ì§€ì‹ ì†ŒìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì´ ì¥ì˜ ëª©í‘œì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72a13cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF íŒŒì¼ ë°œê²¬: ../data/documents/ostep.pdf\n",
      "ğŸ“ íŒŒì¼ í¬ê¸°: 4,147,960 bytes (3.96 MB)\n",
      "ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: 643\n",
      "\n",
      "ğŸ” ì²« 5í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n",
      "  í˜ì´ì§€ 1: OPERATING SYSTEMS THREE EASY PIECES REMZI H. A RPACI -DUSSEAU ANDREA C. A RPACI -DUSSEAU UNIVERSITY OF WISCONSIN â€“M ADISON...\n",
      "  í˜ì´ì§€ 2: ...\n",
      "  í˜ì´ì§€ 3: .. c/circlecopyrt2014 by Arpaci-Dusseau Books, Inc. All rights reserved...\n",
      "  í˜ì´ì§€ 4: ...\n",
      "  í˜ì´ì§€ 5: i To Vedat S. Arpaci, a lifelong inspiration c/circlecopyrt2014, A RPACI -DUSSEAUTHREE EASY PIECES...\n"
     ]
    }
   ],
   "source": [
    "# OSTEP PDF íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "import os\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. OSTEP PDF íŒŒì¼ ê²½ë¡œ í™•ì¸\n",
    "pdf_path = \"../data/documents/ostep.pdf\" \n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬\n",
    "if os.path.exists(pdf_path):\n",
    "    actual_pdf_path = pdf_path\n",
    "\n",
    "if actual_pdf_path:\n",
    "    print(f\"âœ… PDF íŒŒì¼ ë°œê²¬: {actual_pdf_path}\")\n",
    "    \n",
    "    # 2. PDF íŒŒì¼ì˜ ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "    file_size = os.path.getsize(actual_pdf_path)\n",
    "    print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    try:\n",
    "        with open(actual_pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: {num_pages}\")\n",
    "            \n",
    "            # 3. ì²« ë²ˆì§¸ ì±•í„°ì˜ ì œëª©ê³¼ í˜ì´ì§€ ë²”ìœ„ í™•ì¸\n",
    "            print(\"\\nğŸ” ì²« 5í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "            for i in range(min(5, num_pages)):\n",
    "                page = pdf_reader.pages[i]\n",
    "                text = page.extract_text()\n",
    "                # ì²« 200ìë§Œ í‘œì‹œ\n",
    "                preview = text[:200].replace('\\n', ' ').strip()\n",
    "                print(f\"  í˜ì´ì§€ {i+1}: {preview}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PDF íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "        print(\"PyPDF2 ëŒ€ì‹  pdfplumberë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ca8bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ ë¬¸ì„œ íŒŒì‹± ë° ì „ì²˜ë¦¬: êµì¬ PDFë¡œë¶€í„° ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°\n",
    "\n",
    "PDF ë¬¸ì„œëŠ” ì‚¬ëŒì´ ì½ê¸°ì—ëŠ” í¸ë¦¬í•˜ì§€ë§Œ, ê¸°ê³„ê°€ ì²˜ë¦¬í•˜ê¸°ì—ëŠ” ë³µì¡í•œ í˜•ì‹ì…ë‹ˆë‹¤. PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, í°íŠ¸ ì •ë³´, ë ˆì´ì•„ì›ƒ ë©”íƒ€ë°ì´í„° ë“±ì„ í¬í•¨í•˜ê³  ìˆì–´, ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë§Œì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤. Pythonì˜ PyPDF2, pdfplumber, pymupdf ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ì¶œ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì£¼ìš” ë¬¸ì œëŠ” í˜ì´ì§€ ë¨¸ë¦¬ê¸€(header), ë°”ë‹¥ê¸€(footer), ìª½ë²ˆí˜¸, ê°ì£¼ ë“± ë³¸ë¬¸ì´ ì•„ë‹Œ ìš”ì†Œë“¤ì´ í•¨ê»˜ ì¶”ì¶œëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ì€ ë‚˜ì¤‘ì— ê²€ìƒ‰ í’ˆì§ˆì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤. ì •ê·œ í‘œí˜„ì‹(regex)ì´ë‚˜ íŒ¨í„´ ë§¤ì¹­ì„ í†µí•´ ë°˜ë³µë˜ëŠ” ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ íŒ¨í„´ì„ ì°¾ì•„ ì œê±°í•˜ê±°ë‚˜, í˜ì´ì§€ ë²ˆí˜¸ì™€ ê°™ì€ ìˆ«ìë§Œ ìˆëŠ” ì¤„ì„ ê±¸ëŸ¬ë‚´ëŠ” ë°©ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ PDF ì¶”ì¶œ ì‹œ ì¤„ë°”ê¿ˆì´ë‚˜ í•˜ì´í”ˆ ì²˜ë¦¬ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. PDFì—ì„œëŠ” ë‹¨ì–´ê°€ ì¤„ ëì—ì„œ í•˜ì´í”ˆìœ¼ë¡œ ë¶„ë¦¬ë˜ëŠ” ê²½ìš°ê°€ ë§ê³ , ë‹¨ë½ êµ¬ë¶„ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•˜ì´í”ˆìœ¼ë¡œ ëë‚˜ëŠ” ì¤„ì„ ë‹¤ìŒ ì¤„ê³¼ ì—°ê²°í•˜ê³ , ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ë‹¨ë½ êµ¬ë¶„ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ë“±ì˜ í›„ì²˜ë¦¬ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbc0a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...\n",
      "ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: 643\n",
      "\n",
      "ğŸ“„ í˜ì´ì§€ 1 (ì •ë¦¬ ì „):\n",
      "  ì›ë³¸ ê¸¸ì´: 122ì\n",
      "  ì •ë¦¬ í›„: 86ì\n",
      "  ë¯¸ë¦¬ë³´ê¸°: REMZI H. A RPACI -DUSSEAU ANDREA C. A RPACI -DUSSEAU UNIVERSITY OF WISCONSIN â€“M ADISON...\n",
      "\n",
      "ğŸ“„ í˜ì´ì§€ 3 (ì •ë¦¬ ì „):\n",
      "  ì›ë³¸ ê¸¸ì´: 71ì\n",
      "  ì •ë¦¬ í›„: 71ì\n",
      "  ë¯¸ë¦¬ë³´ê¸°: .. c/circlecopyrt2014 by Arpaci-Dusseau Books, Inc. All rights reserved...\n",
      "\n",
      "âœ… ì¶”ì¶œ ì™„ë£Œ: 609í˜ì´ì§€\n",
      "ğŸ“Š ì „ì²´ ë¬¸ì ìˆ˜: 1,302,220ì\n",
      "ğŸ“Š í˜ì´ì§€ë‹¹ í‰ê· : 2138ì\n",
      "\n",
      "ğŸ’¾ pages_data ë³€ìˆ˜ì— 609ê°œ í˜ì´ì§€ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2/pdfplumber ì‚¬ìš©)\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í˜ì´ì§€ë³„ë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    pages_data = []\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: {num_pages}\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                if text:\n",
    "                    # ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "                    cleaned_text = clean_text(text)\n",
    "                    \n",
    "                    pages_data.append({\n",
    "                        'page_number': page_num,\n",
    "                        'raw_text': text,\n",
    "                        'cleaned_text': cleaned_text,\n",
    "                        'char_count': len(cleaned_text)\n",
    "                    })\n",
    "                    \n",
    "                    if page_num <= 3:  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "                        print(f\"\\nğŸ“„ í˜ì´ì§€ {page_num} (ì •ë¦¬ ì „):\")\n",
    "                        print(f\"  ì›ë³¸ ê¸¸ì´: {len(text)}ì\")\n",
    "                        print(f\"  ì •ë¦¬ í›„: {len(cleaned_text)}ì\")\n",
    "                        print(f\"  ë¯¸ë¦¬ë³´ê¸°: {cleaned_text[:150]}...\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "    return pages_data\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë°”ë‹¥ê¸€/ë¨¸ë¦¬ê¸€/í˜ì´ì§€ ë²ˆí˜¸ ë“±ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. í•˜ì´í”ˆìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ ì—°ê²°\n",
    "    text = re.sub(r'-\\s*\\n', '', text)\n",
    "    \n",
    "    # 2. í˜ì´ì§€ ë²ˆí˜¸ ì œê±° (ìˆ«ìë§Œ ìˆëŠ” ì¤„)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. ë°˜ë³µë˜ëŠ” ë°”ë‹¥ê¸€ / ë¨¸ë¦¬ê¸€ ì œê±°\n",
    "    footer_patterns = [\n",
    "        # \"c/circlecopyrt2014...\" í˜•íƒœ\n",
    "        r'c/circlecopyrt\\s*2014.*?(A\\s*RPACI.*?DUSSEAU.*?THREE\\s*EASY\\s*PIECES)',\n",
    "        r'c/circlecopyrt.*?A\\s*RPACI.*?DUSSEAU.*?THREE\\s*EASY\\s*PIECES',\n",
    "        r'Â©?\\s*2014.*?Arpaci-?Dusseau.*?THREE\\s*EASY\\s*PIECES',\n",
    "        \n",
    "        # \"OPERATING SYSTEMS [VERSION 0.80] WWW .OSTEP .ORG\" í˜•íƒœ\n",
    "        r'OPERATING\\s*SYSTEMS\\s*\\[?VERSION\\s*[\\d\\.]+\\]?\\s*WWW\\s*\\.?OSTEP\\s*\\.?ORG',\n",
    "        \n",
    "        # \"OPERATING SYSTEMS ... THREE EASY PIECES\" í˜•íƒœ\n",
    "        r'OPERATING\\s*SYSTEMS.*?THREE\\s*EASY\\s*PIECES',\n",
    "    ]\n",
    "    \n",
    "    for pattern in footer_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # 4. ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    # 5. ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\t\n",
    "    \n",
    "    return text\t\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if 'actual_pdf_path' in locals() and actual_pdf_path:\n",
    "    print(\"ğŸ”„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...\")\n",
    "    pages_data = extract_text_from_pdf(actual_pdf_path)\n",
    "    \n",
    "    if pages_data:\n",
    "        print(f\"\\nâœ… ì¶”ì¶œ ì™„ë£Œ: {len(pages_data)}í˜ì´ì§€\")\n",
    "        \n",
    "        # ì „ì²´ í†µê³„\n",
    "        total_chars = sum(page['char_count'] for page in pages_data)\n",
    "        avg_chars = total_chars / len(pages_data)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë¬¸ì ìˆ˜: {total_chars:,}ì\")\n",
    "        print(f\"ğŸ“Š í˜ì´ì§€ë‹¹ í‰ê· : {avg_chars:.0f}ì\")\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì„¤ì •\n",
    "        print(f\"\\nğŸ’¾ pages_data ë³€ìˆ˜ì— {len(pages_data)}ê°œ í˜ì´ì§€ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë¨¼ì € PDF íŒŒì¼ì„ ì°¾ì•„ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036c06f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ ë° êµ¬ì¡°í™”: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨/ì„¹ì…˜ ë‹¨ìœ„ë¡œ ì •ëˆí•˜ê³  ê³„ì¸µ êµ¬ì¡° ì •ë³´ ë³´ì¡´\n",
    "\n",
    "ì¶”ì¶œëœ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë¬¸ë§¥ ì •ë³´ê°€ ì†ì‹¤ë˜ê³  ê²€ìƒ‰ íš¨ìœ¨ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ êµ¬ì¡°í™”í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤. OSTEPê³¼ ê°™ì€ êµì¬ëŠ” ì±•í„°(Chapter), ì„¹ì…˜(Section), ì„œë¸Œì„¹ì…˜(Subsection) ë“±ì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì´ëŸ¬í•œ êµ¬ì¡° ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ ë‚˜ì¤‘ì— ë” ì •í™•í•œ ê²€ìƒ‰ê³¼ ë‹µë³€ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "êµ¬ì¡°í™” ì‘ì—…ì€ ì œëª© íŒ¨í„´ì„ ì¸ì‹í•˜ëŠ” ê²ƒì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤. êµì¬ì˜ ì±•í„° ì œëª©ì€ ë³´í†µ \"Chapter 1: Introduction\"ê³¼ ê°™ì€ í˜•ì‹ì„ ê°€ì§€ë©°, ì„¹ì…˜ ì œëª©ì€ \"1.1 Virtualizing the CPU\"ì²˜ëŸ¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŒ¨í„´ì„ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì°¾ì•„ë‚´ì–´ ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ê° í…ìŠ¤íŠ¸ ì¡°ê°ì´ ì–´ëŠ ì±•í„°, ì–´ëŠ ì„¹ì…˜ì— ì†í•˜ëŠ”ì§€ ë©”íƒ€ë°ì´í„°ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤. ì—°ì†ëœ ë‘ ê°œ ì´ìƒì˜ ì¤„ë°”ê¿ˆì„ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ì¸ì‹í•˜ê³ , ê° ë¬¸ë‹¨ì„ ë…ë¦½ì ì¸ í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” JSONì´ë‚˜ CSV ê°™ì€ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬, ê° í…ìŠ¤íŠ¸ ì¡°ê°ê³¼ í•¨ê»˜ ì±•í„°ëª…, ì„¹ì…˜ëª…, í˜ì´ì§€ ë²ˆí˜¸ ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ë³´ê´€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”íƒ€ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— ë‹µë³€ ìƒì„± ì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8014a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° íŒŒì‹± ì‹œì‘...\n",
      "\n",
      "âœ… íŒŒì‹± ì™„ë£Œ: 605ê°œ ë¬¸ë‹¨\n",
      "ğŸ“Š ë°œê²¬ëœ ì±•í„° ìˆ˜: 0\n",
      "ğŸ“Š ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: 0\n",
      "\n",
      "ğŸ“š ì±•í„°ë³„ ë¬¸ë‹¨ ìˆ˜:\n",
      "ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: output/ostep_structured.json\n",
      "ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: output/ostep_structured.csv\n",
      "\n",
      "ğŸ” ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ ë¬¸ë‹¨):\n",
      "\n",
      "ë¬¸ë‹¨ 1:\n",
      "  ì±•í„°: None (Chapter None)\n",
      "  ì„¹ì…˜: None (None)\n",
      "  í˜ì´ì§€: 1\n",
      "  ë‚´ìš©: REMZI H. A RPACI -DUSSEAU ANDREA C. A RPACI -DUSSEAU UNIVERSITY OF WISCONSIN â€“M ADISON...\n",
      "\n",
      "ë¬¸ë‹¨ 2:\n",
      "  ì±•í„°: None (Chapter None)\n",
      "  ì„¹ì…˜: None (None)\n",
      "  í˜ì´ì§€: 3\n",
      "  ë‚´ìš©: .. c/circlecopyrt2014 by Arpaci-Dusseau Books, Inc. All rights reserved...\n",
      "\n",
      "ë¬¸ë‹¨ 3:\n",
      "  ì±•í„°: None (Chapter None)\n",
      "  ì„¹ì…˜: None (None)\n",
      "  í˜ì´ì§€: 7\n",
      "  ë‚´ìš©: Preface To Everyone Welcome to this book! We hope youâ€™ll enjoy reading it as much a s we enjoyed wri...\n",
      "\n",
      "ğŸ’¾ structured_data ë³€ìˆ˜ì— 605ê°œ ë¬¸ë‹¨ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì±•í„°/ì„¹ì…˜ êµ¬ì¡° íŒŒì‹± ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def parse_chapter_structure(pages_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    í˜ì´ì§€ ë°ì´í„°ì—ì„œ ì±•í„°ì™€ ì„¹ì…˜ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    structured_data = []\n",
    "    current_chapter = None\n",
    "    current_section = None\n",
    "    \n",
    "    # ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ë“¤\n",
    "    chapter_pattern = r'^Chapter\\s+(\\d+):\\s*(.+)$'\n",
    "    section_pattern = r'^(\\d+\\.\\d+)\\s+(.+)$'\n",
    "    \n",
    "    for page_data in pages_data:\n",
    "        page_num = page_data['page_number']\n",
    "        text = page_data['cleaned_text']\n",
    "        \n",
    "        # í˜ì´ì§€ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• \n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            # ì±•í„° ì œëª© í™•ì¸\n",
    "            chapter_match = re.match(chapter_pattern, para, re.MULTILINE)\n",
    "            if chapter_match:\n",
    "                chapter_num = chapter_match.group(1)\n",
    "                chapter_title = chapter_match.group(2).strip()\n",
    "                current_chapter = {\n",
    "                    'number': int(chapter_num),\n",
    "                    'title': chapter_title\n",
    "                }\n",
    "                current_section = None\n",
    "                print(f\"ğŸ“š ì±•í„° ë°œê²¬: Chapter {chapter_num}: {chapter_title}\")\n",
    "                continue\n",
    "            \n",
    "            # ì„¹ì…˜ ì œëª© í™•ì¸\n",
    "            section_match = re.match(section_pattern, para, re.MULTILINE)\n",
    "            if section_match:\n",
    "                section_num = section_match.group(1)\n",
    "                section_title = section_match.group(2).strip()\n",
    "                current_section = {\n",
    "                    'number': section_num,\n",
    "                    'title': section_title\n",
    "                }\n",
    "                print(f\"  ğŸ“– ì„¹ì…˜ ë°œê²¬: {section_num} {section_title}\")\n",
    "                continue\n",
    "            \n",
    "            # ì¼ë°˜ ë¬¸ë‹¨ì¸ ê²½ìš°\n",
    "            if len(para) > 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì€ ì œì™¸\n",
    "                structured_data.append({\n",
    "                    'page_number': page_num,\n",
    "                    'chapter_number': current_chapter['number'] if current_chapter else None,\n",
    "                    'chapter_title': current_chapter['title'] if current_chapter else None,\n",
    "                    'section_number': current_section['number'] if current_section else None,\n",
    "                    'section_title': current_section['title'] if current_section else None,\n",
    "                    'content': para,\n",
    "                    'char_count': len(para),\n",
    "                    'word_count': len(para.split())\n",
    "                })\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "def save_structured_data(data: List[Dict], output_dir: str = \"output\"):\n",
    "    \"\"\"\n",
    "    êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ JSONê³¼ CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # JSON ì €ì¥\n",
    "    json_path = os.path.join(output_dir, \"ostep_structured.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {json_path}\")\n",
    "    \n",
    "    # CSV ì €ì¥\n",
    "    csv_path = os.path.join(output_dir, \"ostep_structured.csv\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {csv_path}\")\n",
    "    \n",
    "    return json_path, csv_path\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if 'pages_data' in locals() and pages_data:\n",
    "    print(\"ğŸ”„ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° íŒŒì‹± ì‹œì‘...\")\n",
    "    structured_data = parse_chapter_structure(pages_data)\n",
    "    \n",
    "    if structured_data:\n",
    "        print(f\"\\nâœ… íŒŒì‹± ì™„ë£Œ: {len(structured_data)}ê°œ ë¬¸ë‹¨\")\n",
    "        \n",
    "        # í†µê³„ ì •ë³´\n",
    "        chapters = set(item['chapter_number'] for item in structured_data if item['chapter_number'])\n",
    "        sections = set(item['section_number'] for item in structured_data if item['section_number'])\n",
    "        \n",
    "        print(f\"ğŸ“Š ë°œê²¬ëœ ì±•í„° ìˆ˜: {len(chapters)}\")\n",
    "        print(f\"ğŸ“Š ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: {len(sections)}\")\n",
    "        \n",
    "        # ì±•í„°ë³„ ë¬¸ë‹¨ ìˆ˜\n",
    "        chapter_counts = {}\n",
    "        for item in structured_data:\n",
    "            if item['chapter_number']:\n",
    "                ch_num = item['chapter_number']\n",
    "                chapter_counts[ch_num] = chapter_counts.get(ch_num, 0) + 1\n",
    "        \n",
    "        print(f\"\\nğŸ“š ì±•í„°ë³„ ë¬¸ë‹¨ ìˆ˜:\")\n",
    "        for ch_num in sorted(chapter_counts.keys()):\n",
    "            print(f\"  Chapter {ch_num}: {chapter_counts[ch_num]}ê°œ ë¬¸ë‹¨\")\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥\n",
    "        json_path, csv_path = save_structured_data(structured_data)\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "        print(f\"\\nğŸ” ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ ë¬¸ë‹¨):\")\n",
    "        for i, item in enumerate(structured_data[:3]):\n",
    "            print(f\"\\në¬¸ë‹¨ {i+1}:\")\n",
    "            print(f\"  ì±•í„°: {item['chapter_title']} (Chapter {item['chapter_number']})\")\n",
    "            print(f\"  ì„¹ì…˜: {item['section_title']} ({item['section_number']})\")\n",
    "            print(f\"  í˜ì´ì§€: {item['page_number']}\")\n",
    "            print(f\"  ë‚´ìš©: {item['content'][:100]}...\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ structured_data ë³€ìˆ˜ì— {len(structured_data)}ê°œ ë¬¸ë‹¨ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âŒ êµ¬ì¡° íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë¨¼ì € PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e257f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ PDF ë³¸ë¬¸ ì¶”ì¶œ ì‹¤ìŠµ: ì˜ˆì‹œ êµì¬ PDFë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì–»ê³  ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±\n",
    "\n",
    "ì‹¤ì œ OSTEP PDF íŒŒì¼ì„ ëŒ€ìƒìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê³¼ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤. ë¨¼ì € PDF íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„ íƒí•˜ê³ , ê°„ë‹¨í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì—¬ í•œ ì±•í„°ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•´ë´…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"Introduction to Operating Systems\" ì±•í„°ë¥¼ ì„ íƒí•˜ì—¬, í•´ë‹¹ ì±•í„°ì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ìœ¡ì•ˆìœ¼ë¡œ ê²€í† í•˜ë©´ì„œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ë¶€ë¶„ì„ íŒŒì•…í•©ë‹ˆë‹¤. í˜ì´ì§€ ë²ˆí˜¸ê°€ ë³¸ë¬¸ì— ì„ì—¬ ìˆëŠ”ì§€, ë¨¸ë¦¬ê¸€ì´ë‚˜ ë°”ë‹¥ê¸€ì´ ë°˜ë³µë˜ëŠ”ì§€, í‘œë‚˜ ê·¸ë¦¼ì˜ ìº¡ì…˜ì´ ì–´ë–»ê²Œ ì¶”ì¶œë˜ëŠ”ì§€ ë“±ì„ í™•ì¸í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ê·œ í‘œí˜„ì‹ì´ë‚˜ ë¬¸ìì—´ ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤. PDF íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜, íŠ¹ì • íŒ¨í„´ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜, ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ ë“±ì„ ë³„ë„ë¡œ êµ¬í˜„í•˜ì—¬ ì¡°í•©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì±•í„°ì— ë™ì¼í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì ìš©í•´ë³´ê³ , ê° ì±•í„°ë³„ë¡œ ì¶”ì¶œ í’ˆì§ˆì´ ì¼ê´€ë˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd56dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ì „ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "ğŸ”„ OSTEP PDF ì²˜ë¦¬ ì‹œì‘: ../data/documents/ostep.pdf\n",
      "ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: 643\n",
      "\n",
      "ğŸ“„ í˜ì´ì§€ 1 (ì •ë¦¬ ì „):\n",
      "  ì›ë³¸ ê¸¸ì´: 122ì\n",
      "  ì •ë¦¬ í›„: 86ì\n",
      "  ë¯¸ë¦¬ë³´ê¸°: REMZI H. A RPACI -DUSSEAU ANDREA C. A RPACI -DUSSEAU UNIVERSITY OF WISCONSIN â€“M ADISON...\n",
      "\n",
      "ğŸ“„ í˜ì´ì§€ 3 (ì •ë¦¬ ì „):\n",
      "  ì›ë³¸ ê¸¸ì´: 71ì\n",
      "  ì •ë¦¬ í›„: 71ì\n",
      "  ë¯¸ë¦¬ë³´ê¸°: .. c/circlecopyrt2014 by Arpaci-Dusseau Books, Inc. All rights reserved...\n",
      "ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: output/ostep_structured.json\n",
      "ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: output/ostep_structured.csv\n",
      "âœ… ì „ì²˜ë¦¬ ì„±ê³µ!\n",
      "  ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: 609ê°œ\n",
      "  ğŸ“ ì¶”ì¶œëœ ë¬¸ë‹¨: 605ê°œ\n",
      "  ğŸ’¾ JSON íŒŒì¼: output/ostep_structured.json\n",
      "  ğŸ’¾ CSV íŒŒì¼: output/ostep_structured.csv\n",
      "\n",
      "ğŸ“š ì±•í„°ë³„ ìš”ì•½:\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:\n",
      "  'process' ê²€ìƒ‰ ê²°ê³¼: 239ê°œ ë¬¸ë‹¨\n",
      "    ìƒ˜í”Œ: v To Educators If you are an instructor or professor who wishes to use this bo ok, please feel free ...\n",
      "  'memory' ê²€ìƒ‰ ê²°ê³¼: 270ê°œ ë¬¸ë‹¨\n",
      "    ìƒ˜í”Œ: v To Educators If you are an instructor or professor who wishes to use this bo ok, please feel free ...\n",
      "  'scheduling' ê²€ìƒ‰ ê²°ê³¼: 78ê°œ ë¬¸ë‹¨\n",
      "    ìƒ˜í”Œ: xii CONTENTS 5 Interlude: Process API 35 5.1 Thefork() System Call . . . . . . . . . . . . . . . . ....\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class OSTEPPreprocessor:\n",
    "    \"\"\"\n",
    "    OSTEP PDF ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í†µí•© í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.pages_data = []\n",
    "        self.structured_data = []\n",
    "    \n",
    "    def process_pdf(self) -> Dict:\n",
    "        \"\"\"\n",
    "        PDF íŒŒì¼ì„ ì™„ì „íˆ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ OSTEP PDF ì²˜ë¦¬ ì‹œì‘: {self.pdf_path}\")\n",
    "        \n",
    "        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        self.pages_data = extract_text_from_pdf(self.pdf_path)\n",
    "        if not self.pages_data:\n",
    "            return {\"success\": False, \"error\": \"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\"}\n",
    "        \n",
    "        # 2. êµ¬ì¡° íŒŒì‹±\n",
    "        self.structured_data = parse_chapter_structure(self.pages_data)\n",
    "        if not self.structured_data:\n",
    "            return {\"success\": False, \"error\": \"êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨\"}\n",
    "        \n",
    "        # 3. ê²°ê³¼ ì €ì¥\n",
    "        output_dir = \"output\"\n",
    "        json_path, csv_path = save_structured_data(self.structured_data, output_dir)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"pages_count\": len(self.pages_data),\n",
    "            \"paragraphs_count\": len(self.structured_data),\n",
    "            \"json_path\": json_path,\n",
    "            \"csv_path\": csv_path\n",
    "        }\n",
    "    \n",
    "    def get_chapter_summary(self) -> Dict:\n",
    "        \"\"\"\n",
    "        ì±•í„°ë³„ ìš”ì•½ ì •ë³´ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        if not self.structured_data:\n",
    "            return {}\n",
    "        \n",
    "        chapter_info = {}\n",
    "        for item in self.structured_data:\n",
    "            if item['chapter_number']:\n",
    "                ch_num = item['chapter_number']\n",
    "                if ch_num not in chapter_info:\n",
    "                    chapter_info[ch_num] = {\n",
    "                        'title': item['chapter_title'],\n",
    "                        'paragraphs': 0,\n",
    "                        'total_chars': 0,\n",
    "                        'sections': set()\n",
    "                    }\n",
    "                \n",
    "                chapter_info[ch_num]['paragraphs'] += 1\n",
    "                chapter_info[ch_num]['total_chars'] += item['char_count']\n",
    "                \n",
    "                if item['section_number']:\n",
    "                    chapter_info[ch_num]['sections'].add(item['section_number'])\n",
    "        \n",
    "        # setì„ listë¡œ ë³€í™˜\n",
    "        for ch_info in chapter_info.values():\n",
    "            ch_info['sections'] = list(ch_info['sections'])\n",
    "        \n",
    "        return chapter_info\n",
    "    \n",
    "    def search_content(self, query: str, chapter_num: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        íŠ¹ì • ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for item in self.structured_data:\n",
    "            if chapter_num and item['chapter_number'] != chapter_num:\n",
    "                continue\n",
    "                \n",
    "            if query_lower in item['content'].lower():\n",
    "                results.append(item)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def test_preprocessing():\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª ì „ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš© PDF ê²½ë¡œë“¤\n",
    "    pdf_path =  \"../data/documents/ostep.pdf\"\n",
    "    # ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "    preprocessor = OSTEPPreprocessor(pdf_path)\n",
    "    result = preprocessor.process_pdf()\n",
    "    \n",
    "    if not result[\"success\"]:\n",
    "        print(f\"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"âœ… ì „ì²˜ë¦¬ ì„±ê³µ!\")\n",
    "    print(f\"  ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {result['pages_count']}ê°œ\")\n",
    "    print(f\"  ğŸ“ ì¶”ì¶œëœ ë¬¸ë‹¨: {result['paragraphs_count']}ê°œ\")\n",
    "    print(f\"  ğŸ’¾ JSON íŒŒì¼: {result['json_path']}\")\n",
    "    print(f\"  ğŸ’¾ CSV íŒŒì¼: {result['csv_path']}\")\n",
    "    \n",
    "    # ì±•í„°ë³„ ìš”ì•½\n",
    "    print(f\"\\nğŸ“š ì±•í„°ë³„ ìš”ì•½:\")\n",
    "    chapter_summary = preprocessor.get_chapter_summary()\n",
    "    for ch_num in sorted(chapter_summary.keys()):\n",
    "        ch_info = chapter_summary[ch_num]\n",
    "        print(f\"  Chapter {ch_num}: {ch_info['title']}\")\n",
    "        print(f\"    - ë¬¸ë‹¨ ìˆ˜: {ch_info['paragraphs']}ê°œ\")\n",
    "        print(f\"    - ì´ ë¬¸ì ìˆ˜: {ch_info['total_chars']:,}ì\")\n",
    "        print(f\"    - ì„¹ì…˜ ìˆ˜: {len(ch_info['sections'])}ê°œ\")\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:\")\n",
    "    test_queries = [\"process\", \"memory\", \"scheduling\"]\n",
    "    for query in test_queries:\n",
    "        results = preprocessor.search_content(query)\n",
    "        print(f\"  '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ë‹¨\")\n",
    "        if results:\n",
    "            sample = results[0]\n",
    "            print(f\"    ìƒ˜í”Œ: {sample['content'][:100]}...\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    test_preprocessing()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
