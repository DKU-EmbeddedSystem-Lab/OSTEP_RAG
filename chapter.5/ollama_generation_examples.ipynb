{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab_intro",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: Colab에서 Ollama로 간단 Q&A\n",
    "\n",
    "Colab 런타임에서 Ollama를 설치·구동하고, `llama3.1:8b` 모델로 사용자의 영어 질문을 한 번에 생성(비스트리밍)하여 출력합니다.\n",
    "\n",
    "\n",
    "## 📚 학습 목표\n",
    "- Ollama 로컬 REST API(`/api/version`, `/api/generate`) 호출 흐름 이해\n",
    "- 최소 셋업(설치 → 서버 구동 → 모델 pull) 절차 습득\n",
    "- 단일 질의 입력 → 비스트리밍 단일 응답 출력 패턴 익히기\n",
    "\n",
    "\n",
    "## 📋 실습 구성\n",
    "1) Google Colab 환경 설정: Ollama 설치, 서버 백그라운드 실행, 모델 pull\n",
    "2) 헬스 체크: `/api/version`으로 서버 상태 확인\n",
    "3) 헬퍼 함수: `ollama_generate(prompt)` 래퍼\n",
    "4) 단일 생성: `input()`으로 질문 받고 즉시 응답 출력\n",
    "\n",
    "> 참고: 본 예제는 RAG/대화/스트리밍/파라미터 튜닝을 제외한 최소 구성입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4591a88",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1️⃣ 실행 전 준비\n",
    "\n",
    "이 셀에서는 Colab 환경에서 Ollama를 설치하고 서버를 백그라운드로 구동한 뒤, 사용할 모델을 내려받습니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- Ollama 설치 스크립트 실행\n",
    "- `ollama serve` 백그라운드 구동(대기 포함)\n",
    "- `llama3.1:8b` 모델 pull\n",
    "\n",
    "**실행 결과:**\n",
    "- 설치 완료 메시지와 함께 서버/모델 정보가 출력됩니다.\n",
    "- 로컬 서버 엔드포인트: `http://localhost:11434`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#  Google Colab 환경 설정\n",
    "#    - Ollama 설치, 서버 백그라운드 실행, 모델 pull\n",
    "# ========================================\n",
    "import subprocess, time\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Ollama 서버 백그라운드 실행\n",
    "print(\"Starting Ollama server ...\")\n",
    "ollama_process = subprocess.Popen([\"ollama\", \"serve\"],\n",
    "                                  stdout=subprocess.DEVNULL,\n",
    "                                  stderr=subprocess.DEVNULL)\n",
    "# 서버 시작 대기\n",
    "time.sleep(5)\n",
    "\n",
    "# 모델 다운로드 (llama3.1:8b)\n",
    "!ollama pull llama3.1:8b\n",
    "\n",
    "print(\"\\n✅ Setup done.\")\n",
    "print(\"   Ollama server: http://localhost:11434\")\n",
    "print(\"   Model: llama3.1:8b (non-streaming)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f932c7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2️⃣ Quick Health Check\n",
    "\n",
    "이 셀에서는 Ollama 서버의 상태를 간단히 확인합니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- `/api/version`에 GET 요청하여 정상 응답 여부 확인\n",
    "\n",
    "**실행 결과:**\n",
    "- 정상: 버전 JSON 출력(`{version: ...}`)\n",
    "- 오류: 예외 메시지 문자열 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2️⃣ Quick Health Check — /api/version\n",
    "# ========================================\n",
    "import requests\n",
    "\n",
    "def ollama_ok(host: str = \"http://localhost:11434\"):\n",
    "    try:\n",
    "        r = requests.get(host + \"/api/version\", timeout=5)\n",
    "        r.raise_for_status()\n",
    "        return True, r.json()\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok, info = ollama_ok()\n",
    "print(\"Server ready:\", ok)\n",
    "print(\"Version or error:\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36439f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3️⃣ Helper Functions\n",
    "\n",
    "이 셀에서는 단일(turn) 생성용 래퍼 함수를 정의합니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- `ollama_generate(prompt)`: `/api/generate`에 비스트리밍 요청\n",
    "- 고정 파라미터(temperature/top_p/num_predict/repeat_penalty) 사용\n",
    "\n",
    "**실행 결과:**\n",
    "- 성공 시 `response` 텍스트 반환\n",
    "- 실패 시 `[Error] ...` 메시지 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab430bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3️⃣ Helper — Single-turn generation wrapper\n",
    "#    - ollama_generate(prompt): 비스트리밍 단일 생성 호출\n",
    "# ========================================\n",
    "import requests, json\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "\n",
    "# Fixed defaults for simple non-streaming Q&A\n",
    "_TEMPERATURE = 0.7\n",
    "_TOP_P = 0.9\n",
    "_NUM_PREDICT = 256\n",
    "_REPEAT_PENALTY = 1.1\n",
    "\n",
    "def ollama_generate(prompt: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": _TEMPERATURE,\n",
    "            \"top_p\": _TOP_P,\n",
    "            \"num_predict\": _NUM_PREDICT,\n",
    "            \"repeat_penalty\": _REPEAT_PENALTY,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=600)\n",
    "        r.raise_for_status()\n",
    "        data = r.json() if isinstance(r.json(), dict) else json.loads(r.text)\n",
    "        return data.get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d2375",
   "metadata": {},
   "source": [
    "---\n",
    "## 4️⃣ Single-turn Generation\n",
    "\n",
    "이 셀에서는 사용자의 영어 질문을 입력받아 단일 응답을 출력합니다.\n",
    "\n",
    "**주요 내용:**\n",
    "- `input()`으로 프롬프트를 받음\n",
    "- `ollama_generate()`로 단일(비스트리밍) 생성 수행\n",
    "\n",
    "**사용 방법:**\n",
    "- 프롬프트에 영어 문장을 입력 후 Enter를 누르면 답변이 출력됩니다.\n",
    "- 빈 입력 시 안내 메시지를 출력하고 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4️⃣ Single-turn Generation — input() → generate → print\n",
    "# ========================================\n",
    "# Enter an English question and get a single response\n",
    "user_prompt = input(\"Enter your question (English): \")\n",
    "if user_prompt.strip():\n",
    "    print(ollama_generate(user_prompt))\n",
    "else:\n",
    "    print(\"[Info] Empty prompt.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
