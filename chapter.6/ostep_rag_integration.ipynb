{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0760dc",
   "metadata": {},
   "source": [
    "# Chapter 6: OSTEP RAG 통합 (검색→프롬프트→생성)\n",
    "\n",
    "이 노트북은 Chapter 2–5에서 생성한 산출물(청크/임베딩/FAISS 인덱스)을 재사용하여, 검색→프롬프트→생성까지 단일 RAG 파이프라인을 시연합니다.\n",
    "\n",
    "## 📚 학습 목표\n",
    "- 기존 산출물 로드만으로 RAG 통합 파이프라인 구성\n",
    "- FAISS HNSW 인덱스를 이용한 Top-K 검색(Top-K=10)\n",
    "- Ollama `llama3.1`로 구조화 프롬프트 기반 응답 생성\n",
    "\n",
    "## 📋 실습 구성\n",
    "1) 설정/하이퍼파라미터 정의(경로/모델/검색 파라미터)\n",
    "2) 산출물 로드(청크 JSON, 인덱스/메타)\n",
    "3) 임베딩 모델 로드(`all-MiniLM-L6-v2`)\n",
    "4) 검색 함수 정의(retrieve)\n",
    "5) 프롬프트/생성(헬퍼 함수)\n",
    "6) 통합 함수(rag_answer)\n",
    "7) 데모 실행(질문→답변+출처)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60435f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1️⃣ 설정 / 하이퍼파라미터 / 의존성 임포트\n",
    "#   - 경로/모델/검색 파라미터는 상단 변수에서 통일 관리\n",
    "# ========================================\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 재현성\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 디바이스\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 경로 (Chapter 4 산출물 기준)\n",
    "INDEX_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw.index\"\n",
    "INDEX_META_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw_metadata.json\"\n",
    "CHUNK_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\"\n",
    "\n",
    "# 검색 하이퍼파라미터\n",
    "TOP_K = 10\n",
    "EF_SEARCH = 64           # HNSW efSearch (검색 품질/속도 트레이드오프)\n",
    "MIN_SCORE = 0.0          # 필터 임계값 (0.0이면 필터 없음)\n",
    "\n",
    "# 임베딩 모델 (쿼리 전용)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "NORMALIZE = True         # 코사인 유사도 기반 점수화를 위해 정규화 사용\n",
    "\n",
    "# LLM(Ollama) 하이퍼파라미터\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.2\n",
    "TOP_P = 0.9\n",
    "MAX_TOKENS = 512\n",
    "REPEAT_PENALTY = 1.1\n",
    "\n",
    "print(f\"🔧 Device: {DEVICE}\")\n",
    "print(f\"📁 Index: {INDEX_FILE}\")\n",
    "print(f\"📁 Chunks: {CHUNK_FILE}\")\n",
    "print(f\"🔎 TOP_K={TOP_K}, efSearch={EF_SEARCH}\")\n",
    "print(f\"🧠 Embed: {EMBED_MODEL}\")\n",
    "print(f\"🤖 LLM: {LLM_MODEL} @ {OLLAMA_HOST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c3723",
   "metadata": {},
   "source": [
    "---\n",
    "## 2️⃣ 산출물 로드\n",
    "\n",
    "Chapter 2–4에서 생성한 청크 JSON과 FAISS 인덱스/메타 정보를 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a668a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청크 로드\n",
    "with open(CHUNK_FILE, 'r', encoding='utf-8') as f:\n",
    "    CHUNKS = json.load(f)\n",
    "print(f\"✓ Chunks loaded: {len(CHUNKS)} entries\")\n",
    "\n",
    "# 인덱스/메타 로드\n",
    "INDEX = faiss.read_index(INDEX_FILE)\n",
    "print(f\"✓ Index loaded: {INDEX.ntotal} vectors\")\n",
    "\n",
    "with open(INDEX_META_FILE, 'r', encoding='utf-8') as f:\n",
    "    INDEX_META = json.load(f)\n",
    "print(\"Index metadata:\")\n",
    "for k, v in INDEX_META.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# efSearch 적용\n",
    "faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", EF_SEARCH)\n",
    "print(f\"efSearch set to {EF_SEARCH}\")\n",
    "\n",
    "# 샘플 출력\n",
    "print(\"\\nSample chunk:\")\n",
    "sample = CHUNKS[0]\n",
    "for key, value in sample.items():\n",
    "    if key == 'text':\n",
    "        print(f\"  {key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b40f8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3️⃣ 임베딩 모델 로드 (쿼리 전용)\n",
    "\n",
    "`sentence-transformers/all-MiniLM-L6-v2`를 로드하여 쿼리를 임베딩합니다. 검색 점수는 정규화된 L2 거리로부터 유사도(≈ 1 - distance/2)로 변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"Loading model: {EMBED_MODEL} on {DEVICE}\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL, device=\"cpu\" if DEVICE==\"cpu\" else DEVICE)\n",
    "print(\"✓ Embed model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85407e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 4️⃣ 검색 함수 정의\n",
    "\n",
    "쿼리 → 임베딩 → FAISS HNSW 검색 → 점수 변환 → 상위 K개의 컨텍스트를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa76eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    index: int\n",
    "    score: float\n",
    "    chunk: Dict[str, Any]\n",
    "\n",
    "def _distance_to_similarity(distances: np.ndarray) -> np.ndarray:\n",
    "    # 정규화된 벡터의 L2 거리 → 유사도 근사: 1 - d/2\n",
    "    return 1.0 - (distances / 2.0)\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K, ef_search: int = EF_SEARCH, min_score: float = MIN_SCORE) -> List[RetrievedChunk]:\n",
    "    faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", ef_search)\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=NORMALIZE).astype('float32')\n",
    "    D, I = INDEX.search(q_emb, k)\n",
    "    scores = _distance_to_similarity(D[0])\n",
    "\n",
    "    results: List[RetrievedChunk] = []\n",
    "    for rank, (idx, sc) in enumerate(zip(I[0], scores)):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        if sc < min_score:\n",
    "            continue\n",
    "        results.append(RetrievedChunk(index=idx, score=float(sc), chunk=CHUNKS[idx]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f3295",
   "metadata": {},
   "source": [
    "---\n",
    "## 5️⃣ 프롬프트/생성 헬퍼 함수\n",
    "\n",
    "Chapter 5 스타일의 `ollama_generate()`와 `build_structured_prompt()`를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ef7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_generate(prompt: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"num_predict\": MAX_TOKENS,\n",
    "            \"repeat_penalty\": REPEAT_PENALTY,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=600)\n",
    "        r.raise_for_status()\n",
    "        data = r.json() if isinstance(r.json(), dict) else json.loads(r.text)\n",
    "        return data.get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {e}\"\n",
    "\n",
    "\n",
    "def build_structured_prompt(\n",
    "    role: str,\n",
    "    goal: str,\n",
    "    constraints: str,\n",
    "    format_spec: str,\n",
    "    context: List[str],\n",
    "    question: str,\n",
    ") -> str:\n",
    "    context_text = \"\\n\\n\".join([f\"[Context {i+1}]\\n{chunk}\" for i, chunk in enumerate(context)])\n",
    "    prompt = f\"\"\"Role:\n",
    "{role}\n",
    "\n",
    "Goal:\n",
    "{goal}\n",
    "\n",
    "Constraints:\n",
    "{constraints}\n",
    "\n",
    "Format:\n",
    "{format_spec}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Please answer the question considering the role, goal, constraints, format, and context provided above.\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4168bb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 6️⃣ 통합 RAG 함수\n",
    "\n",
    "검색 → 컨텍스트 선택 → 구조화 프롬프트 생성 → Ollama 호출 → 응답/출처 반환.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea47c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_context_from_results(results: List[RetrievedChunk], max_chars: int = 600) -> List[str]:\n",
    "    formatted = []\n",
    "    for r in results:\n",
    "        text = r.chunk.get(\"text\", \"\")\n",
    "        snippet = text[:max_chars]\n",
    "        title = r.chunk.get(\"chapter_title\") or r.chunk.get(\"chapter_id\") or \"\"\n",
    "        prefix = f\"[Chapter: {title}]\\nScore: {r.score:.4f}\\n\"\n",
    "        formatted.append(prefix + snippet)\n",
    "    return formatted\n",
    "\n",
    "ROLE = \"\"\"You are an expert tutor specializing in operating system concepts.\n",
    "You explain concepts clearly and concisely so students can understand easily,\n",
    "and you use concrete examples to illustrate concepts.\"\"\"\n",
    "\n",
    "GOAL = \"\"\"Provide evidence-based answers to questions based on the provided context.\n",
    "Find and cite relevant information from the context in your answers,\n",
    "and explicitly state when information is not available in the context.\"\"\"\n",
    "\n",
    "CONSTRAINTS = \"\"\"1. Your answer must be based on the provided context.\n",
    "2. If you can find relevant information in the context, you must include 1-3 citations.\n",
    "3. If relevant information is not available in the context, you must explicitly state 'The information is not available in the provided context.'\n",
    "4. Any speculative or uncertain content must be clearly marked.\"\"\"\n",
    "\n",
    "FORMAT_SPEC = \"\"\"Please respond in the following JSON format:\n",
    "\n",
    "{\n",
    "  \"answer\": \"Answer to the question (1-2 paragraphs)\",\n",
    "  \"citations\": [\n",
    "    {\"chunk_id\": 1, \"quote\": \"quoted text\"},\n",
    "    {\"chunk_id\": 2, \"quote\": \"quoted text\"}\n",
    "  ],\n",
    "  \"summary\": \"Summary (1-2 sentences)\"\n",
    "}\"\"\"\n",
    "\n",
    "def rag_answer(question: str, top_k: int = TOP_K) -> Dict[str, Any]:\n",
    "    # 1) Retrieve\n",
    "    results = retrieve(question, k=top_k, ef_search=EF_SEARCH, min_score=MIN_SCORE)\n",
    "\n",
    "    # 2) Build prompt (use top-3 for compactness)\n",
    "    top_context = _format_context_from_results(results[:3])\n",
    "    prompt = build_structured_prompt(\n",
    "        role=ROLE,\n",
    "        goal=GOAL,\n",
    "        constraints=CONSTRAINTS,\n",
    "        format_spec=FORMAT_SPEC,\n",
    "        context=top_context,\n",
    "        question=question,\n",
    "    )\n",
    "\n",
    "    # 3) Generate\n",
    "    answer = ollama_generate(prompt)\n",
    "\n",
    "    # 4) Sources\n",
    "    sources = []\n",
    "    for r in results[:3]:\n",
    "        preview = (r.chunk.get(\"text\", \"\")[:80] + \"...\") if r.chunk.get(\"text\") else \"\"\n",
    "        sources.append({\n",
    "            \"score\": round(r.score, 4),\n",
    "            \"chapter_title\": r.chunk.get(\"chapter_title\"),\n",
    "            \"chunk_id\": r.chunk.get(\"chunk_id\"),\n",
    "            \"preview\": preview,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"top_k\": top_k,\n",
    "        \"sources\": sources,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a3628",
   "metadata": {},
   "source": [
    "---\n",
    "## 7️⃣ 데모 실행: 질문 → 답변 + 출처\n",
    "\n",
    "`rag_answer()`를 호출해 통합 동작을 확인합니다. Ollama 서버가 실행 중이어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ca69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does the operating system handle memory virtualization?\"\n",
    "result = rag_answer(question)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Question:\")\n",
    "print(result[\"question\"]) \n",
    "print(\"=\"*80)\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"]) \n",
    "print(\"\\n[Sources]\")\n",
    "for i, s in enumerate(result[\"sources\"], 1):\n",
    "    print(f\"{i}. ({s['score']:.4f}) [{s.get('chapter_title')}] {s.get('preview')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a562a5d",
   "metadata": {},
   "source": [
    "---\n",
    "## 8️⃣ (선택) 간단 검색 적합성 확인\n",
    "\n",
    "`test_queries.json`에서 일부 질의를 불러와 검색 상위 결과의 챕터 타이틀을 미리보기로 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71198c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUERIES = \"/home/kbs1102/workspace/OSTEP_RAG/data/documents/test_queries.json\"\n",
    "\n",
    "try:\n",
    "    with open(TEST_QUERIES, 'r', encoding='utf-8') as f:\n",
    "        test_qs = json.load(f)\n",
    "    print(f\"Loaded {len(test_qs)} test queries. Showing first 3...\")\n",
    "    for q in test_qs[:3]:\n",
    "        rs = retrieve(q, k=5)\n",
    "        print(\"\\nQ:\", q)\n",
    "        for i, r in enumerate(rs, 1):\n",
    "            title = r.chunk.get('chapter_title')\n",
    "            print(f\"  {i}. {title} (score={r.score:.4f})\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[Info] test_queries.json not found. Skipping optional check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4bf5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 9️⃣ 인터랙티브 Q&A (실시간 질의)\n",
    "\n",
    "사용자로부터 입력을 받아 `rag_answer()`로 답변을 생성합니다. 종료하려면 `exit`를 입력하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        user_q = input(\"Enter your question (type 'exit' to quit): \").strip()\n",
    "        if not user_q:\n",
    "            print(\"[Info] Empty input. Try again.\")\n",
    "            continue\n",
    "        if user_q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"[Info] Bye.\")\n",
    "            break\n",
    "        result = rag_answer(user_q)\n",
    "        print(\"=\"*80)\n",
    "        print(\"Question:\")\n",
    "        print(result[\"question\"]) \n",
    "        print(\"=\"*80)\n",
    "        print(\"Answer:\")\n",
    "        print(result[\"answer\"]) \n",
    "        print(\"\\n[Sources]\")\n",
    "        for i, s in enumerate(result.get(\"sources\", []), 1):\n",
    "            print(f\"{i}. ({s['score']:.4f}) [{s.get('chapter_title')}] {s.get('preview')}\")\n",
    "        print(\"\\n\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[Info] Interrupted by user.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
